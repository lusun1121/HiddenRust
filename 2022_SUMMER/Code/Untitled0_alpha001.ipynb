{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0_alpha001.ipynb","provenance":[],"authorship_tag":"ABX9TyMA3bnz3V0O2YQ/msEUtasg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tTM-UtbdzHc8","executionInfo":{"status":"ok","timestamp":1656082479955,"user_tz":300,"elapsed":20005,"user":{"displayName":"Lu Sun","userId":"12220989488268625125"}},"outputId":"dbf1152b-4717-4e14-f4b8-51f3a830f6bf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pickle\n","import numpy as np\n","filename = '/content/drive/MyDrive/2020_WINTER/Rust/data_final/table2_sample_final.txt'\n","with open(filename,'rb') as fl:\n","  haha = pickle.load(fl)\n","#np.sum(haha['res'][1]['action'][:,0]!=0)\n","#haha['res'][1]['belief'][30:60,0]\n","#np.sum(haha['res'][1]['hidden'][:,0]==0)/3000\n","[(i,np.sum(haha['res'][1]['state'][:,0]==i)) for i in np.unique(haha['res'][1]['state'][:,0])]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0sSzZkXCr43","executionInfo":{"status":"ok","timestamp":1656083984728,"user_tz":300,"elapsed":164,"user":{"displayName":"Lu Sun","userId":"12220989488268625125"}},"outputId":"32838ea6-585d-4f6c-85b6-c6b9ca9f099a"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 1),\n"," (2, 1),\n"," (3, 1),\n"," (4, 1),\n"," (5, 1),\n"," (6, 2),\n"," (7, 1),\n"," (8, 1),\n"," (9, 1),\n"," (10, 1),\n"," (11, 2),\n"," (12, 1),\n"," (13, 1),\n"," (14, 1),\n"," (16, 1),\n"," (18, 1),\n"," (20, 1),\n"," (22, 1),\n"," (23, 1),\n"," (25, 1),\n"," (27, 1),\n"," (29, 1),\n"," (30, 1),\n"," (31, 1),\n"," (33, 1),\n"," (34, 1),\n"," (35, 1),\n"," (36, 1),\n"," (38, 1),\n"," (40, 1),\n"," (42, 1),\n"," (43, 2),\n"," (44, 1),\n"," (45, 1),\n"," (46, 1),\n"," (47, 1),\n"," (49, 1),\n"," (51, 1),\n"," (52, 1),\n"," (54, 1),\n"," (56, 1),\n"," (57, 1),\n"," (59, 1),\n"," (60, 1),\n"," (61, 1),\n"," (63, 1),\n"," (64, 1),\n"," (65, 2),\n"," (67, 1),\n"," (69, 1),\n"," (71, 1),\n"," (73, 1),\n"," (74, 1),\n"," (75, 2),\n"," (76, 1),\n"," (77, 1),\n"," (78, 1),\n"," (79, 1),\n"," (80, 1),\n"," (81, 1),\n"," (82, 3),\n"," (83, 1),\n"," (84, 1),\n"," (85, 1),\n"," (86, 2),\n"," (87, 1),\n"," (88, 1),\n"," (89, 1),\n"," (90, 1),\n"," (91, 1),\n"," (92, 1),\n"," (93, 2),\n"," (94, 1),\n"," (95, 1),\n"," (96, 1),\n"," (97, 3),\n"," (98, 1),\n"," (99, 1),\n"," (100, 2),\n"," (101, 2),\n"," (102, 1),\n"," (103, 1),\n"," (104, 1),\n"," (105, 2),\n"," (107, 1),\n"," (108, 1)]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6XaNc3CzypXo","executionInfo":{"status":"ok","timestamp":1655656128849,"user_tz":300,"elapsed":4607112,"user":{"displayName":"Lu Sun","userId":"12220989488268625125"}},"outputId":"3cd4edf2-9d13-47ec-9b6c-9caf2c0bef1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reminder1: file group4_new_175.csv should be in the same file directory\n","Reminder2: the code will take almost 4 hours\n","->Date generation\n","[0.012430000000000002, 0.0022600000000000003, 0.01164]\n","generate Q\n","generate dataset\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3000/3000 [00:51<00:00, 58.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["->Estimation1 x0 known, alpha =  1.0\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -268933.30741349095\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -268933.30741349095\n","[0.03900001 0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -268933.3014868614\n","[0.039      0.33300001 0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -268933.30737780896\n","[0.039      0.333      0.59000001 0.181      0.757      0.061\n"," 0.949      0.012     ] -268933.30738219916\n","[0.039      0.333      0.59       0.18100001 0.757      0.061\n"," 0.949      0.012     ] -268933.3074024633\n","[0.039      0.333      0.59       0.181      0.75700001 0.061\n"," 0.949      0.012     ] -268933.3074156445\n","[0.039      0.333      0.59       0.181      0.757      0.06100001\n"," 0.949      0.012     ] -268933.3074062741\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.94900001 0.012     ] -268933.3074346598\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.01200001] -268933.3073948919\n","Optimization terminated successfully.    (Exit mode 0)\n","            Current function value: 268933.30741349095\n","            Iterations: 5\n","            Function evaluations: 10\n","            Gradient evaluations: 1\n","     fun: 268933.30741349095\n","     jac: array([-3.97729375e+05, -2.39457812e+03, -2.09995703e+03, -7.40054688e+02,\n","        1.44523438e+02, -4.84316406e+02,  1.42061719e+03, -1.24816016e+03])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012])\n","[1.30515 0.2373  1.2222 ] -160801.03470257486\n","[1.30515 0.2373  1.2222 ] -160801.03470257486\n","[1.30515001 0.2373     1.2222    ] -160801.03474924783\n","[1.30515    0.23730001 1.2222    ] -160801.03470184546\n","[1.30515    0.2373     1.22220001] -160801.03470221208\n","[1.06865089e-05 3.73772109e+01 3.73772110e+01] -265130.76722347195\n","[1.17463607 3.95129109 4.8377011 ] -162198.83918718313\n","[1.28382282 0.84420032 1.81300615] -160757.75563320424\n","[1.28382282 0.84420032 1.81300615] -160757.75563320424\n","[1.28382284 0.84420032 1.81300615] -160757.75565147228\n","[1.28382282 0.84420033 1.81300615] -160757.75563363\n","[1.28382282 0.84420032 1.81300616] -160757.75563395646\n","[1.13382700e+00 1.33235171e-07 1.54814420e-07] -160978.70486832643\n","[1.24066243 0.60128681 1.29132461] -160712.6584992112\n","[1.24066243 0.60128681 1.29132461] -160712.6584992112\n","[1.24066245 0.60128681 1.29132461] -160712.65849206413\n","[1.24066243 0.60128682 1.29132461] -160712.65849971378\n","[1.24066243 0.60128681 1.29132463] -160712.65849966128\n","[1.26686923e+00 9.95762336e-07 9.99745445e-07] -160807.59611575035\n","[1.24630742 0.47176892 1.01317152] -160704.1948118552\n","[1.24630742 0.47176892 1.01317152] -160704.1948118552\n","[1.24630743 0.47176892 1.01317152] -160704.19481303418\n","[1.24630742 0.47176894 1.01317152] -160704.1948120294\n","[1.24630742 0.47176892 1.01317154] -160704.19481186644\n","[1.23595779e+00 9.99984028e-07 1.18102716e+00] -160704.1180201178\n","[1.24106778 0.2329301  1.09815068] -160702.60768849\n","[1.24106778 0.2329301  1.09815068] -160702.60768849\n","[1.24106779 0.2329301  1.09815068] -160702.60768794\n","[1.24106778 0.23293012 1.09815068] -160702.60768851996\n","[1.24106778 0.2329301  1.0981507 ] -160702.60768853745\n","[1.22870902 0.30527043 0.80277474] -160708.0214871629\n","[1.2398319  0.24016413 1.06861309] -160702.63057217337\n","[1.24069895 0.23508897 1.08933571] -160702.602627537\n","[1.24069895 0.23508897 1.08933571] -160702.602627537\n","[1.24069897 0.23508897 1.08933571] -160702.60262675278\n","[1.24069895 0.23508899 1.08933571] -160702.6026275698\n","[1.24069895 0.23508897 1.08933572] -160702.6026275779\n","[1.24157867 0.21124202 1.04742212] -160702.5759276728\n","[1.24157867 0.21124202 1.04742212] -160702.5759276728\n","[1.24157868 0.21124202 1.04742212] -160702.57592823976\n","[1.24157867 0.21124203 1.04742212] -160702.57592764767\n","[1.24157867 0.21124202 1.04742213] -160702.5759276418\n","[1.24122807 0.22199786 1.06550894] -160702.5415176983\n","[1.24122807 0.22199786 1.06550894] -160702.5415176983\n","[1.24122809 0.22199786 1.06550894] -160702.54151769794\n","[1.24122807 0.22199788 1.06550894] -160702.54151769832\n","[1.24122807 0.22199786 1.06550895] -160702.5415176984\n","     fun: 160702.5415176983\n","     jac: array([-0.0234375 ,  0.00195312,  0.0078125 ])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 47\n","     nit: 8\n","    njev: 8\n","  status: 0\n"," success: True\n","       x: array([1.24122807, 0.22199786, 1.06550894])\n","->Estimation1 x0 known, alpha =  0.1\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -268933.30741349095\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -268933.30741349095\n","[0.03900001 0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -268933.3014868614\n","[0.039      0.33300001 0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -268933.30737780896\n","[0.039      0.333      0.59000001 0.181      0.757      0.061\n"," 0.949      0.012     ] -268933.30738219916\n","[0.039      0.333      0.59       0.18100001 0.757      0.061\n"," 0.949      0.012     ] -268933.3074024633\n","[0.039      0.333      0.59       0.181      0.75700001 0.061\n"," 0.949      0.012     ] -268933.3074156445\n","[0.039      0.333      0.59       0.181      0.757      0.06100001\n"," 0.949      0.012     ] -268933.3074062741\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.94900001 0.012     ] -268933.3074346598\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.01200001] -268933.3073948919\n","Optimization terminated successfully.    (Exit mode 0)\n","            Current function value: 268933.30741349095\n","            Iterations: 5\n","            Function evaluations: 10\n","            Gradient evaluations: 1\n","     fun: 268933.30741349095\n","     jac: array([-3.97729375e+05, -2.39457812e+03, -2.09995703e+03, -7.40054688e+02,\n","        1.44523438e+02, -4.84316406e+02,  1.42061719e+03, -1.24816016e+03])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012])\n","[0.130515 0.02373  0.12222 ] -160801.03470257486\n","[0.130515 0.02373  0.12222 ] -160801.03470257486\n","[0.13051501 0.02373    0.12222   ] -160801.03516930516\n","[0.130515   0.02373001 0.12222   ] -160801.0346952817\n","[0.130515   0.02373    0.12222001] -160801.0346989473\n","[1.31463480e-02 9.99998366e+01 9.99999398e+01] -1969340.9790459503\n","[ 0.11877813 10.02134066 10.10999198] -283978.74202617\n","[0.12934131 1.02349107 1.1209972 ] -165667.7155530311\n","[0.13039763 0.12370611 0.22209772] -160827.85716885192\n","[0.13047149 0.06079402 0.15924754] -160787.79546115155\n","[0.13047149 0.06079402 0.15924754] -160787.79546115155\n","[0.1304715  0.06079402 0.15924754] -160787.79584557362\n","[0.13047149 0.06079403 0.15924754] -160787.7954596279\n","[0.13047149 0.06079402 0.15924756] -160787.79546368725\n","[0.09338762 0.03026649 0.03028272] -163192.63465374013\n","[0.12512057 0.05638913 0.14063892] -160712.73617468725\n","[0.12512057 0.05638913 0.14063892] -160712.73617468725\n","[0.12512059 0.05638913 0.14063892] -160712.73617887706\n","[0.12512057 0.05638915 0.14063892] -160712.73617854898\n","[0.12512057 0.05638913 0.14063894] -160712.73617943\n","[1.23449059e-01 3.50595586e-05 2.20070741e-05] -160762.77353173497\n","[0.12466545 0.04104486 0.10235128] -160703.6475292344\n","[0.12466545 0.04104486 0.10235128] -160703.6475292344\n","[0.12466546 0.04104486 0.10235128] -160703.6475505417\n","[0.12466545 0.04104487 0.10235128] -160703.64753034373\n","[0.12466545 0.04104486 0.10235129] -160703.6475291514\n","[1.23424539e-01 1.02988440e-06 1.20859025e-01] -160704.65009098814\n","[0.12414979 0.02398925 0.1100421 ] -160702.61714335514\n","[0.12414979 0.02398925 0.1100421 ] -160702.61714335514\n","[0.12414981 0.02398925 0.1100421 ] -160702.61714007243\n","[0.12414979 0.02398927 0.1100421 ] -160702.61714368558\n","[0.12414979 0.02398925 0.11004212] -160702.61714385403\n","[1.25315518e-01 1.00000000e-06 1.24362023e-01] -160709.06024471103\n","[0.12426637 0.02159043 0.11147409] -160702.6534972575\n","[0.12417659 0.02343786 0.11037126] -160702.61358592793\n","[0.12417659 0.02343786 0.11037126] -160702.61358592793\n","[0.1241766  0.02343786 0.11037126] -160702.61358512344\n","[0.12417659 0.02343788 0.11037126] -160702.6135861808\n","[0.12417659 0.02343786 0.11037127] -160702.61358641705\n","[0.12341738 0.00678692 0.00678692] -160752.14142510816\n","[0.12410067 0.02177277 0.10001282] -160702.72342207894\n","[0.12414742 0.02279823 0.10639217] -160702.5431332957\n","[0.12414742 0.02279823 0.10639217] -160702.5431332957\n","[0.12414744 0.02279823 0.10639217] -160702.54313455828\n","[0.12414742 0.02279825 0.10639217] -160702.54313332136\n","[0.12414742 0.02279823 0.10639219] -160702.54313328487\n","[0.1241141  0.02147766 0.10683421] -160702.54287706016\n","[0.12412993 0.02210518 0.10662416] -160702.54171002546\n","[0.12412993 0.02210518 0.10662416] -160702.54171002546\n","[0.12412995 0.02210518 0.10662416] -160702.5417106282\n","[0.12412993 0.0221052  0.10662416] -160702.54171001015\n","[0.12412993 0.02210518 0.10662417] -160702.5417100249\n","[0.1241214  0.02221631 0.1065255 ] -160702.5415236758\n","[0.1241214  0.02221631 0.1065255 ] -160702.5415236758\n","[0.12412142 0.02221631 0.1065255 ] -160702.54152356638\n","[0.1241214  0.02221633 0.1065255 ] -160702.54152367858\n","[0.1241214  0.02221631 0.10652551] -160702.54152367587\n","[0.12412272 0.02219926 0.10654096] -160702.54151730315\n","[0.12412272 0.02219926 0.10654096] -160702.54151730315\n","[0.12412273 0.02219926 0.10654096] -160702.54151730312\n","[0.12412272 0.02219927 0.10654096] -160702.5415173031\n","[0.12412272 0.02219926 0.10654098] -160702.54151730315\n","     fun: 160702.54151730315\n","     jac: array([-0.00195312, -0.00390625,  0.        ])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 62\n","     nit: 10\n","    njev: 10\n","  status: 0\n"," success: True\n","       x: array([0.12412272, 0.02219926, 0.10654096])\n","->Estimation1 x0 known, alpha =  0.01\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -268933.30741349095\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -268933.30741349095\n","[0.03900001 0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -268933.3014868614\n","[0.039      0.33300001 0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -268933.30737780896\n","[0.039      0.333      0.59000001 0.181      0.757      0.061\n"," 0.949      0.012     ] -268933.30738219916\n","[0.039      0.333      0.59       0.18100001 0.757      0.061\n"," 0.949      0.012     ] -268933.3074024633\n","[0.039      0.333      0.59       0.181      0.75700001 0.061\n"," 0.949      0.012     ] -268933.3074156445\n","[0.039      0.333      0.59       0.181      0.757      0.06100001\n"," 0.949      0.012     ] -268933.3074062741\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.94900001 0.012     ] -268933.3074346598\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.01200001] -268933.3073948919\n","Optimization terminated successfully.    (Exit mode 0)\n","            Current function value: 268933.30741349095\n","            Iterations: 5\n","            Function evaluations: 10\n","            Gradient evaluations: 1\n","     fun: 268933.30741349095\n","     jac: array([-3.97729375e+05, -2.39457812e+03, -2.09995703e+03, -7.40054688e+02,\n","        1.44523438e+02, -4.84316406e+02,  1.42061719e+03, -1.24816016e+03])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012])\n","[0.0130515 0.002373  0.012222 ] -160801.03470257486\n","[0.0130515 0.002373  0.012222 ] -160801.03470257486\n","[0.01305151 0.002373   0.012222  ] -160801.0393699271\n","[0.0130515  0.00237301 0.012222  ] -160801.03462964343\n","[0.0130515  0.002373   0.01222201] -160801.03466629924\n","[ -2.85796092 100.04374611 100.02130049] -inf\n","[-0.27404974 10.00651031 10.01312985] -8194853.516359865\n","[-0.01565862  1.00278673  1.01231278] -601487.1409334447\n","[0.01018049 0.10241437 0.11223108] -169534.94816526503\n","[0.0127644  0.01237714 0.02222291] -160799.92824203224\n","[0.01290697 0.00740921 0.01725658] -160761.73914129924\n","[0.01290697 0.00740921 0.01725658] -160761.73914129924\n","[0.01290698 0.00740921 0.01725658] -160761.74167472817\n","[0.01290697 0.00740922 0.01725658] -160761.73916219446\n","[0.01290697 0.00740921 0.01725659] -160761.73919800716\n","     fun: 160761.73914129924\n","     jac: array([170015.53710938,   1402.25390625,   3805.60351562])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 15\n","     nit: 6\n","    njev: 2\n","  status: 0\n"," success: True\n","       x: array([0.01290697, 0.00740921, 0.01725658])\n","[(        state  action\n","0          94       0\n","1          95       0\n","2          96       1\n","3           0       0\n","4           1       1\n","...       ...     ...\n","299995      0       0\n","299996      2       0\n","299997      4       0\n","299998      6       0\n","299999      8       1\n","\n","[300000 rows x 2 columns], {'state': array([[ 94,  95,  96, ...,   6,   7,   9],\n","       [150, 151, 152, ...,   3,   5,   6],\n","       [ 86,   0,   0, ...,   5,   7,   9],\n","       ...,\n","       [ 65,  66,  67, ...,   2,   0,   2],\n","       [170, 171, 172, ...,   4,   6,   7],\n","       [150, 151, 152, ...,   4,   6,   8]]), 'action': array([[0, 0, 1, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [1, 1, 0, ..., 0, 0, 0],\n","       ...,\n","       [0, 0, 0, ..., 1, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 1],\n","       [0, 0, 0, ..., 0, 0, 1]]), 'hidden': array([[1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0]]), 'belief': array([[0.        , 0.012     , 0.01697964, ..., 0.94326908, 0.83630062,\n","        0.93041337],\n","       [0.        , 0.012     , 0.01697964, ..., 0.84699123, 0.9318202 ,\n","        0.81537345],\n","       [0.        , 1.        , 1.        , ..., 0.94326908, 0.94320959,\n","        0.94320319],\n","       ...,\n","       [1.        , 0.949     , 0.84699123, ..., 0.949     , 1.        ,\n","        0.949     ],\n","       [1.        , 0.949     , 0.84699123, ..., 0.84699123, 0.9318202 ,\n","        0.81537345],\n","       [1.        , 0.949     , 0.84699123, ..., 0.94382256, 0.94326908,\n","        0.94320959]]), 'pis': 0.5}),      fun: 268933.30741349095\n","     jac: array([-3.97729375e+05, -2.39457812e+03, -2.09995703e+03, -7.40054688e+02,\n","        1.44523438e+02, -4.84316406e+02,  1.42061719e+03, -1.24816016e+03])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012]),      fun: 160702.5415176983\n","     jac: array([-0.0234375 ,  0.00195312,  0.0078125 ])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 47\n","     nit: 8\n","    njev: 8\n","  status: 0\n"," success: True\n","       x: array([1.24122807, 0.22199786, 1.06550894]),      fun: 268933.30741349095\n","     jac: array([-3.97729375e+05, -2.39457812e+03, -2.09995703e+03, -7.40054688e+02,\n","        1.44523438e+02, -4.84316406e+02,  1.42061719e+03, -1.24816016e+03])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012]),      fun: 160702.54151730315\n","     jac: array([-0.00195312, -0.00390625,  0.        ])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 62\n","     nit: 10\n","    njev: 10\n","  status: 0\n"," success: True\n","       x: array([0.12412272, 0.02219926, 0.10654096]),      fun: 268933.30741349095\n","     jac: array([-3.97729375e+05, -2.39457812e+03, -2.09995703e+03, -7.40054688e+02,\n","        1.44523438e+02, -4.84316406e+02,  1.42061719e+03, -1.24816016e+03])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012]),      fun: 160761.73914129924\n","     jac: array([170015.53710938,   1402.25390625,   3805.60351562])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 15\n","     nit: 6\n","    njev: 2\n","  status: 0\n"," success: True\n","       x: array([0.01290697, 0.00740921, 0.01725658])]\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat Nov 14 00:47:19 2020\n","11:44\n","@author: Sunlu\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","import scipy.optimize as opt\n","from scipy.optimize import minimize #scipy version = 1.4.1\n","import time as tm\n","#import matplotlib.pyplot as plt\n","#import seaborn as sns\n","import pandas as pd\n","from scipy.misc import derivative\n","import warnings\n","\n","from tqdm import tqdm\n","warnings.filterwarnings(\"ignore\")\n","# warnings.filterwarnings(\"error\")\n","#\n","\n","#%%\n","# def partial_derivative(func, point):\n","#     tol = len(point)\n","#     def wraps(x,var):\n","#         args = point.copy()#[:]\n","#         args[var] = x\n","#         #print(args)\n","#         return func(args)\n","    \n","#     pd = []\n","#     for var in range(tol):\n","#         wraps_new = lambda x: wraps(x,var)\n","#         pd.append(derivative(wraps_new,point[var]))\n","        \n","#     return \n","\n","class FullLikelihood(object):\n","    def __init__(self, data, Y, X,belief0, dim_x, dim_z,time, nbus, alpha,hide_state = True, disp_status = False):\n","        \"\"\"\n","        A statistics workbench used to evaluate the cost parameters underlying \n","        a bus replacement pattern by a forward-looking agent.\n","        \n","        Input:\n","            \n","            * data: a Pandas dataframe, which contains:\n","                -Y: the name of the column containing the dummy exogenous \n","                    variable (here, the choice)\n","                -X: the name of the column containing the endogenous variable \n","                    (here, the state of the bus)\n","            \n","            * dim_x: The number of belif. Start from 0 to 1.\n","            \n","            * dim_z: The number of observation state bins.\n","            \n","            * time: The length of time horizon of the observations.           \n","            \n","            * nbus: The number of buses.\n","            \n","            * hide_state: Default is True. Decide to use Hidden model or not.\n","            \n","            * disp_status: Default is False. Print the grid of opt process.\n","        \"\"\"        \n","        self.data = np.array(data)\n","        self.alpha = alpha\n","        #self.endog = data.loc[:, Y].values\n","        self.exog = data.loc[:, X].values\n","        self.dim = dim_x     \n","        self.S = dim_z \n","        self.time = time\n","        self.nbus = nbus\n","        self.hide_state = hide_state\n","        self.disp_status = disp_status\n","        self.belief0 = belief0\n","        # To speed up computations and avoid loops when computing the log \n","        # likelihood, we create a few useful matrices here:\n","        self.N = data.loc[:, Y].values.shape[0]\n","        self.Dlength = self.S*self.dim\n","        #length = self.Dlength\n","        \n","        # A (length x 2) matrix indicating the blief of a bus at observation \n","        # state i \n","        self.D = [(i,j/(self.dim-1)) for i in range(self.S) \n","                                     for j in range(self.dim)]\n","        \n","\n","        \n","        # A (length x length) matrix indicating the probability of a bus \n","        # transitioning from a state z to a state z' with centain belif  \n","        # self.regen_mat = np.vstack((np.zeros((self.dim-1, self.Dlength)),\n","        #                             np.ones((1,self.Dlength)),\n","        #                             np.zeros((self.Dlength-self.dim, self.Dlength))))\n","        \n","        # A (2xN) matrix indicating with a dummy the decision taken by the \n","        # agent for each time/bus observation (replace or maintain)        \n","        #self.dec_mat = np.vstack(((1-data.loc[:, Y].values), data.loc[:, Y].values))\n","               \n","    def trans(self, theta2,p0,p1):\n","        \"\"\"\n","        This function generate the transition matrix\n","        \n","        Input:\n","            \n","            * theta2: The true state transition probability\n","            \n","            * p0: The transition probability when true state s = 0 (good)\n","            \n","            * p1: The transition probability when true state s = 1 (bad)\n","        Output:\n","        \n","            * trans_mat: A (length x length) matrix indicating the probability \n","                        of a bus transitioning from a obsevation state z to a \n","                        observation state z' with centain belif in good or bad\n","                        true state s.\n","        \"\"\"\n","        \n","        S = self.S\n","        D = self.D\n","        dim = self.dim\n","        length = self.Dlength\n","        \n","        p_0 = np.array([p0[0],p1[0]])\n","        p_0 = np.around(p_0,3)\n","        p_1 = np.array([p0[1],p1[1]])\n","        p_1 = np.around(p_1,3)\n","        p_2 = np.array([p0[2],p1[2]])\n","        p_2 = np.around(p_2,3)\n","        p_3 = 1-p_0-p_1-p_2\n","        \n","        #self.P = P = np.array((p_0,p_1,p_2,p_3))\n","        P = np.array((p_0,p_1,p_2,p_3))\n","        trans_mat = np.zeros((length, length))\n","        new_x0 = lambda x: np.dot(p_0*np.array([x,1-x]),theta2) / np.dot(p_0,[x,1-x])\n","        new_x1 = lambda x: np.dot(p_1*np.array([x,1-x]),theta2) / np.dot(p_1,[x,1-x])\n","        new_x2 = lambda x: np.dot(p_2*np.array([x,1-x]),theta2) / np.dot(p_2,[x,1-x])\n","        new_x3 = lambda x: np.dot(p_3*np.array([x,1-x]),theta2) / np.dot(p_3,[x,1-x])\n","        \n","        matrix0 = [new_x0(i / (dim-1)) for i in range(dim)]\n","        matrix1 = [new_x1(i / (dim-1)) for i in range(dim)]\n","        matrix2 = [new_x2(i / (dim-1)) for i in range(dim)]\n","        matrix3 = [new_x3(i / (dim-1)) for i in range(dim)]\n","        \n","        #self.matrix = x_matrix = np.vstack((matrix0,matrix1,matrix2,matrix3))\n","        x_matrix = np.vstack((matrix0,matrix1,matrix2,matrix3))\n","        \n","        for i in range(length):\n","            \n","            z_cur = D[i][0]\n","            x_cur = D[i][1]\n","            \n","            for j in range(4):\n","                \n","                if self.hide_state == True:\n","                    x_new = x_matrix[j][int(x_cur*(dim-1))]\n","                else:\n","                    x_new = 1\n","                    \n","                x_f = np.floor((self.dim-1)*x_new)/(self.dim-1)\n","                coe = (x_new-x_f)*(self.dim - 1)\n","                \n","                if z_cur + j < S-1:\n","                    ind = D.index((z_cur + j,x_f))\n","                    trans_mat[ind][i] = (1-coe)*np.dot(P[j],[x_cur,1-x_cur])\n","                    \n","                    if x_f != 1:\n","                        trans_mat[ind+1][i] = coe*np.dot(P[j],[x_cur,1-x_cur])\n","                        \n","                elif z_cur + j >= S-1:\n","                    ind = D.index((S-1,x_f))\n","                    trans_mat[ind][i] += (1-coe)*np.dot(P[j],[x_cur,1-x_cur])\n","                    \n","                    if x_f != 1:\n","                        trans_mat[ind+1][i] += coe*np.dot(P[j],[x_cur,1-x_cur])\n","                        \n","                else:\n","                    pass\n","                \n","        return trans_mat\n","    \n","    def belief(self,theta2,p0,p1, path ,x,data_gene_status = False):\n","        \"\"\"\n","        This function return the belief x\n","        \n","        Input:\n","            \n","            * theta2: The true state transition probability\n","            \n","            * p0: The transition probability when s = 0\n","            \n","            * p1: The transition probability when s = 1\n","            \n","            * path: A path of mileage and replacement/maintenance record data\n","        \n","        Output:\n","            \n","            * x: A list with length 'time'. Record belief of each state z\n","        \"\"\"\n","        if data_gene_status:\n","            #path = [[zold,action],[znew,0]]\n","            length = 2\n","            #x = [path[0][2]] #return(xold,xnew)           \n","        \n","        else:\n","            length = self.time\n","            #x = [1]\n","        x = [x]\n","        \n","        p_0 = np.array([p0[0],p1[0]])\n","        p_1 = np.array([p0[1],p1[1]])\n","        p_2 = np.array([p0[2],p1[2]])\n","        p_3 = 1-p_0-p_1-p_2\n","        \n","        for i in range(length-1):\n","            \n","            if path[i][1] == 1:\n","                x_new = 1\n","                x.append(x_new)\n","                \n","            else:\n","                x_cur= np.array([x[-1],1-x[-1]])\n","                gap = path[i+1][0]-path[i][0]\n","                \n","                if gap == 0:\n","                    v1 = np.dot(p_0,x_cur)\n","                    v2 = np.dot(p_0,np.diag(x_cur))\n","                    v3 = np.dot(v2,theta2)\n","                    try:                    \n","                        x_new = v3/v1\n","                    except RuntimeWarning:\n","                        x_new = v3/(v1+np.finfo(float).eps)\n","                    x.append(x_new)\n","                    \n","                elif gap == 1:\n","                    v1 = np.dot(p_1,x_cur)\n","                    v2 = np.dot(p_1,np.diag(x_cur))\n","                    v3 = np.dot(v2,theta2)\n","                    try:                    \n","                        x_new = v3/v1\n","                    except RuntimeWarning:\n","                        x_new = v3/(v1+np.finfo(float).eps)\n","                    x.append(x_new)\n","                    \n","                elif gap == 2:\n","                    v1 = np.dot(p_2,x_cur)\n","                    v2 = np.dot(p_2,np.diag(x_cur))\n","                    v3 = np.dot(v2,theta2)\n","                    try:                    \n","                        x_new = v3/v1\n","                    except RuntimeWarning:\n","                        x_new = v3/(v1+np.finfo(float).eps)\n","                    x.append(x_new)\n","                    \n","                else:\n","                    v1 = np.dot(p_3,x_cur)\n","                    v2 = np.dot(p_3,np.diag(x_cur))\n","                    v3 = np.dot(v2,theta2)\n","                    try:                    \n","                        x_new = v3/v1\n","                    except RuntimeWarning:\n","                        x_new = v3/(v1+np.finfo(float).eps)\n","                    x.append(x_new)\n","        return x\n","    \n","    def simulation(self, dim_x0,SampleSize, TimePeriod, reward = [1.243,0.226,1.164],theta2 = [0.949,0.012],\\\n","                    p0 = [0.039,0.333,0.590],p1=[0.181,0.757,0.061],rand_status = False):\n","        reward = [rw*self.alpha for rw in reward]\n","        print(reward)\n","        condition = np.zeros([SampleSize,TimePeriod],dtype = int) #hidden state: 0,1\n","        state = np.zeros([SampleSize,TimePeriod],dtype = int ) #mileage: 175:0,...,174\n","        x = np.ones([SampleSize,TimePeriod])#belief: s = 0 prob # 删除\n","        if rand_status: # s0 z0 x0\n","            x[:,0] = np.kron(np.linspace(0, 1,num= dim_x0),np.ones(int(SampleSize/dim_x0)))\n","        \n","            #x[:,0] = np.kron(np.random.uniform(size = dim_x0),np.ones(int(SampleSize/dim_x0)))\n","            state[:,0] = np.random.choice(np.arange(175,dtype= int),size = SampleSize,p=np.ones(175)/175)#uniform(0,20)\n","            pis =0.5#0.9509143407122231#np.random.uniform()\n","            condition[:,0] = np.random.choice([0,1],size = SampleSize, p = [pis,1-pis])#0000\n","        else:\n","            pis = 1\n","        action = np.zeros([SampleSize,TimePeriod],dtype = int)\n","        rho = 1\n","        print(\"generate Q\")\n","        Q = self.fl_costs( reward, theta2, p0, p1 ) # Q function Q(x,a) --> interpolation \n","        pchoice =self.choice_prob(rho,Q) # z = 0, x= 0, 0.1, 0.2,....,1, z = 1, x = 0,...,1 pi\n","        print(\"generate dataset\")\n","        for nbus in tqdm(range(SampleSize)):\n","            for t in range(TimePeriod-1):\n","                path = np.zeros([2,2])\n","                path[0,0] = state[nbus,t]\n","                #path[0,2] = x[nbus,t] \n","                \n","                fl =  int(np.floor(x[nbus,t]*(self.dim-1)))\n","                cl =  int(np.ceil(x[nbus,t]*(self.dim-1)))\n","                coef = x[nbus,t] * (self.dim-1)-fl\n","                pi0 = pchoice[state[nbus,t] * self.dim + fl,0] * (1-coef) +\\\n","                    pchoice[state[nbus,t] * self.dim + cl,0] *coef\n","                    \n","                action[nbus,t] = np.random.choice([0,1],p=[pi0,1-pi0])\n","                path[0,1] = action[nbus,t]\n","                \n","                if action[nbus,t]==1:\n","                    state[nbus,t+1] = 0\n","                    condition[nbus,t+1] = 0\n","                    path[1,0] = state[nbus,t+1]\n","                else:\n","                    if condition[nbus,t] == 0:\n","                        state[nbus,t+1] = state[nbus,t] + np.random.choice([0,1,2,3],p=[p0[0],p0[1],p0[2],1-np.sum(p0)])\n","                        if state[nbus,t+1]>self.S-1:\n","                            state[nbus,t+1] = self.S-1\n","                            \n","                        condition[nbus,t+1] = np.random.choice([0,1],p=[theta2[0],1-theta2[0]])\n","                        path[1,0] = state[nbus,t+1]  \n","                    else:\n","                        state[nbus,t+1] = state[nbus,t] + np.random.choice([0,1,2,3],p=[p1[0],p1[1],p1[2],1-np.sum(p1)])\n","                        if state[nbus,t+1]>self.S-1:\n","                            state[nbus,t+1] = self.S-1\n","                        condition[nbus,t+1] = np.random.choice([0,1],p=[theta2[1],1-theta2[1]])\n","                        path[1,0] = state[nbus,t+1]\n","                x[nbus,t+1] = self.belief(theta2,p0,p1, path,x[nbus,t] ,data_gene_status = True)[-1]\n","                \n","            fl =  int(np.floor(x[nbus,TimePeriod-1]*(self.dim-1)))\n","            cl =  int(np.ceil(x[nbus,TimePeriod-1]*(self.dim-1)))\n","            coef = x[nbus,TimePeriod-1] * (self.dim-1)-fl\n","            pi0 = pchoice[state[nbus,TimePeriod-1] * self.dim + fl,0] * (1-coef) +\\\n","                pchoice[state[nbus,TimePeriod-1] * self.dim + cl,0] *coef\n","                \n","            action[nbus,TimePeriod-1] = np.random.choice([0,1],p=[pi0,1-pi0])        \n","          \n","        state_new = np.reshape(state,[-1])\n","        action_new = np.reshape(action,[-1])\n","        data = {'state':state_new,'action':action_new}\n","        dat_new = pd.DataFrame(data=data)\n","        \n","        return dat_new,{'state':state,'action':action,'hidden':condition,'belief':x,'pis':pis}\n","    \n","    def myopic_costs(self, params):\n","        \"\"\"\n","        This function computes the myopic expected cost associated with each \n","        decision for each state.\n","\n","        Input:\n","\n","            * params: A vector, to be supplied to the maintenance linear cost \n","            function. The first element of the vector is the replacement cost rc.\n","\n","        Output:\n","            \n","            * A (length x 2) array containing the maintenance and replacement \n","            costs for the z possible states of the bus.\n","        \"\"\"\n","        \n","        length = self.Dlength\n","        D = self.D\n","        rc = params[0]\n","        thetas = np.array(params[1:])\n","        maint_cost = np.array([np.dot(0.001*D[d][0]*thetas,[D[d][1],1-D[d][1]])\n","                               for d in range(0, length)])\n","        repl_cost = [rc for d in range(0, length)]\n","\n","        return  np.vstack((maint_cost, repl_cost)).T\n","     \n","    def fl_costs(self, params, theta2, p0, p1 ,beta=0.9, threading = 1e-3):#beta=0.9999, threading = 3e-3):#beta=0.9999, threading = 0.45):#\n","        \"\"\"\n","        Compute the non-myopic expected value of the agent for each possible \n","        decision and each possible state of the bus, conditional on a vector of \n","        parameters and on the maintenance cost function specified at the \n","        initialization of the DynamicUtility model.\n","\n","        Iterates until the difference in the previously obtained expected value \n","        and the new expected value is smaller than a constant 'threading'.\n","\n","        Input:\n","\n","            * params: A vector params for the cost function\n","\n","            * theta2: Hiden state transition probability\n","\n","            * p0: The transition probability when s = 0\n","\n","            * p1: The transition probability when s = 1\n","\n","            * beta: Default is 0.9999. A discount factor beta (optional)\n","\n","            * threading: Default is 0.45. A convergence threshold (optional)\n","\n","        Output:\n","\n","            * EV_new: A (length x 2) array of forward-looking costs associated \n","                    with each state z and each belief. Definded as Q\n","        \"\"\"\n","\n","        # Initialization of the contraction mapping\n","        #self.EV_myopic =self.myopic_costs(params)\n","        EV_myopic =self.myopic_costs(params)\n","        \n","        EV_new = EV_myopic\n","\n","        EV = np.zeros(EV_new.shape)        \n","        #VError = np.max(abs(EV-EV_new))\n","        VError = np.max(abs(EV-EV_new))/np.max(np.abs(EV))\n","\n","        #print('Initial Error:',VError,np.max(abs(EV_new)))\n","\n","        #self.trans_mat = self.trans(theta2,p0,p1)\n","        trans_mat = self.trans(theta2,p0,p1)\n","        # Contraction mapping Loop\n","        i = 0\n","        length = self.Dlength\n","        \n","        while(VError>threading):         \n","\n","            EV = EV_new\n","            m = EV.min(1).reshape(length, -1)\n","            #ecost = np.log(np.exp(-EV+m).sum(1) )-m.T+0.5772\n","            ecost = self.alpha*(np.log(np.exp((-EV+m)/self.alpha).sum(1) )-m.T/self.alpha+0.5772)\n","          \n","            futil_maint = np.dot(ecost, trans_mat)\n","            futil_repl = np.dot(ecost, np.vstack((np.zeros((self.dim-1, self.Dlength)),\n","                                    np.ones((1,self.Dlength)),\n","                                    np.zeros((self.Dlength-self.dim, self.Dlength)))))\n","            futil = np.vstack((futil_maint, futil_repl)).T\n","            EV_new = EV_myopic - beta * futil\n","\n","            i +=1\n","            # VError = np.max(abs(EV-EV_new))\n","            # print(i,'th Error:',VError,VError/np.max(abs(EV)),np.max(abs(EV_new)))\n","\n","            VError = np.max(abs(EV-EV_new))/np.max(np.abs(EV))\n","            # print(i,'th Error:',VError,np.max(abs(EV-EV_new)),np.max(abs(EV_new)))\n","\n","            if i>1000:\n","                break\n","            \n","        #print('{}, Eorror:'.format(i),VError,np.max(abs(EV_new)))   \n","\n","        return EV_new\n","\n","    def choice_prob(self, rho, cost_array):\n","        \"\"\"\n","        Returns the probability of each choice for each observed state, \n","        conditional on an array of state/decision costs (generated by the \n","        myopic or forward-looking cost functions)\n","        \"\"\"\n","        cost_array = cost_array/rho\n","        cost = cost_array - cost_array.min(1).reshape(self.Dlength, -1)\n","        #util = np.exp(-cost)\n","        util = np.exp(-cost/self.alpha)\n","        pchoice = util / (np.sum(util, 1).reshape(self.Dlength, -1))\n","\n","        return pchoice\n","\n","    # def loglike_full(self,theta2):\n","    #     \"\"\"\n","    #     The log-likelihood of the Dynamic model(theta 2) is estimated.\n","    #     \"\"\"\n","    #     self.theta2 = theta2\n","    #     self.fit_likelihood2()\n","        \n","    #     loglike = -self.fitted2.fun\n","        \n","    #     if self.disp_status:\n","    #         print(theta2,loglike)\n","\n","    #     return -loglike\n","   \n","    def loglike2(self,parameters):\n","        if len(parameters)==8:\n","            theta2 = parameters[6:8]\n","        else:\n","            theta2 = self.theta2\n","        \n","    # def loglike2(self,parameters):\n","    #     \"\"\"\n","    #     The log-likelihood of the Dynamic model(theta 3) is estimated.\n","    #     \"\"\"\n","\n","    #     theta2 = parameters[6:]\n","        p0 = parameters[0:3]\n","        p1 = parameters[3:6]\n","        \n","        prob0 = np.array([p0[0],p1[0]])\n","        prob1 = np.array([p0[1],p1[1]])\n","        prob2 = np.array([p0[2],p1[2]])\n","\n","        prob3 = 1 - prob0 -prob1 - prob2\n","\n","        mileage = self.exog\n","\n","        if self.hide_state == True:\n","            beli = []\n","            \n","            for i in range(self.nbus):\n","                beli = beli + self.belief(theta2,p0,p1,\n","                                self.data[i*self.time:(i+1)*self.time],self.belief0[i])\n","        else:\n","            beli = [1 for s in range(0,self.N)]\n","        \n","        logprob = 0\n","        for i in range(self.N-1):\n","            x_cur = [beli[i], 1-beli[i]]\n","\n","            if mileage[i+1] - mileage[i] == 0:\n","                try:\n","                    logprob += np.log(np.dot(prob0,x_cur))\n","                except RuntimeWarning:\n","                    logprob += np.log(np.finfo(float).eps)\n","            \n","            elif mileage[i+1] - mileage[i] == 1:\n","                try:\n","                    logprob += np.log(np.dot(prob1,x_cur))\n","                except RuntimeWarning:\n","                    logprob += np.log(np.finfo(float).eps)             \n","\n","            elif mileage[i+1] - mileage[i] == 2:\n","                try:\n","                    logprob += np.log(np.dot(prob2,x_cur))\n","                except RuntimeWarning:\n","                    logprob += np.log(np.finfo(float).eps)\n","\n","            elif mileage[i+1] - mileage[i] == 3:\n","                try:\n","                    logprob += np.log(np.dot(prob3,x_cur))\n","                except RuntimeWarning:\n","                    logprob += np.log(np.finfo(float).eps)\n","\n","        #print(f'current likelihood: {logprob}')\n","\n","        if self.disp_status:\n","            print(parameters,logprob)\n","        return -logprob#/self.time/self.nbus\n","\n","    def loglike(self, parameters, p0, p1, theta2):\n","        \"\"\"\n","        The log-likelihood of the reward model is estimated in several steps.\n","\n","        1°) The current parameters are supplied to the contraction mapping \n","            function\n","\n","        2°) The function returns a matrix of decision probabilities for each \n","            state.\n","\n","        3°) This matrix is used to compute the loglikelihood of the \n","            observations\n","\n","        4°) The log-likelihood are then summed accross individuals, and \n","            returned\n","        \"\"\"\n","\n","        params = parameters[0:3]\n","        rho = 1\n","\n","        # p0 = self.fitted2.x[0:3]\n","        # p1 = self.fitted2.x[3:6]\n","        # theta2 = self.theta2\n","        # A (length x 1) matrix        \n","         \n","\n","        beli = []\n","        #self.beli = []\n","\n","        for i in range(self.nbus):\n","            beli = beli + self.belief(theta2,p0,p1,\n","                                        self.data[i*self.time:(i+1)*self.time],self.belief0[i])\n","\n","\n","        # bool_to_int_f = np.array(((np.array((np.ones((self.Dlength,1)) * self.exog.reshape((1,self.N))) == (np.array(self.D).T[0].reshape((self.Dlength,1)) * np.ones((1,self.N))), dtype=int)) + \\\n","        #                           np.array((np.ones((self.Dlength,1)) * np.floor((np.array(self.beli).reshape((1,self.N)))*(self.dim-1)) / (self.dim-1)) == (np.array(self.D).T[1].reshape((self.Dlength,1)) * np.ones((1,self.N))), dtype=int)) == 2, dtype=int)\n","        # bool_to_int_c = np.array(((np.array((np.ones((self.Dlength,1)) * self.exog.reshape((1,self.N))) == (np.array(self.D).T[0].reshape((self.Dlength,1)) * np.ones((1,self.N))), dtype=int)) +\\\n","        #                           np.array((np.ones((self.Dlength,1)) * np.ceil((np.array(self.beli).reshape((1,self.N))) * (self.dim-1)) / (self.dim-1)) == (np.array(self.D).T[1].reshape((self.Dlength,1)) * np.ones((1,self.N))), dtype=int)) == 2, dtype=int)\n","\n","        # #self.state_mat =\n","        \n","        # self.D = [(i,j/(self.dim-1)) for i in range(self.S) \n","        #                              for j in range(self.dim)]\n","        bool_floor = np.int_(np.floor(np.array(beli)*(self.dim-1)))\n","        bool_ceil = np.int_(np.ceil(np.array(beli)*(self.dim-1)))\n","        bool_non0 = np.where((bool_floor-bool_ceil)!=0)\n","        coef_floor = np.zeros(bool_floor.shape)\n","        coef_floor[bool_non0] = (np.array(beli)*(self.dim-1)-bool_ceil)[bool_non0]/(bool_floor-bool_ceil)[bool_non0]\n","        \n","        choiceProb = self.choice_prob(rho,self.fl_costs(params,theta2,p0,p1)) #states,actions\n","        action = np.int_(self.data[:,1])\n","        statef =  np.int_(self.exog*self.dim+bool_floor)\n","        statec = np.int_(self.exog*self.dim+bool_ceil)\n","        logprob = coef_floor * choiceProb[statef,action] + (1-coef_floor)* choiceProb[statec,action] \n","        \n","        logprob = np.sum(np.log(logprob))\n","        \n","        if self.disp_status:\n","            print(parameters,logprob)\n","\n","        #print(f'current likelihood:{parameters} {logprob}')\n","\n","        return -logprob#/self.time/self.nbus\n","    \n","    \n","    \n","    def fit_likelihood2(self):\n","        \"\"\"\n","        estimate the dynamic(theta 3)\n","        \"\"\"\n","        # bounds = [(0.001, 0.998),(0.001, 0.998),(0.001, 0.998),\n","        #           (0.001, 0.998),(0.001, 0.998),(0.001, 0.998),\n","        #           (0,0.99999),(0,0.99999)]\n","        #x0 = [0.1,0.1,0.1,0.1,0.1,0.1, 0.1, 0.1]\n","        x0 = np.array([0.039,0.333,0.590,0.181,0.757,0.061,0.949,0.012])\n","        cons= ({'type': 'ineq', 'fun': lambda x: x[0]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[1]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[2]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[3]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[4]-0.001},\\\n","                {'type': 'ineq', 'fun': lambda x: x[5]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[6]},\\\n","                {'type': 'ineq', 'fun': lambda x: x[7]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[0]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[1]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[2]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[3]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[4]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[5]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.99999-x[6]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.99999-x[7]},\\\n","                {'type': 'ineq', 'fun': lambda x: 1-x[0]-x[1]-x[2]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: 1-x[3]-x[4]-x[5]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: -3*x[0]-2*x[1]-x[2]+3*x[3]+2*x[4]+x[5] })\n","        # x0 = [0.1,0.1,0.1,0.1,0.1,0.1]\n","        # cons= ({'type': 'ineq', 'fun': lambda x: x[0]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: x[1]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: x[2]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: x[3]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: x[4]-0.001},\\\n","        #        {'type': 'ineq', 'fun': lambda x: x[5]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[0]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[1]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[2]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[3]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[4]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[5]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 1-x[0]-x[1]-x[2]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: 1-x[3]-x[4]-x[5]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: -3*x[0]-2*x[1]-x[2]+3*x[3]+2*x[4]+x[5] })\n","        self.fitted2 = minimize(\n","            self.loglike2,\n","            x0=x0,#bounds = bounds,\n","            constraints= cons,\n","            options={'disp': True})\n","        if len(x0)==8:\n","            self.theta2 = self.fitted2.x[6:8]\n","        \n","    def fit_likelihood(self, x0=None, bounds=None,p0 =None,p1=None ):\n","        \"\"\"\n","        estimate the reward parameter\n","        \"\"\"\n","        if bounds == None:\n","            bounds = [(1e-6, 100),(1e-6,100),(1e-6,100)]\n","\n","        if x0 == None:\n","            #x0 = [10,1,1,]\n","            #x0 =  [9.36, 0.94, 0.94] # To speed up, choose a closer initial point\n","            #x0 = [0.1, 0.1, 0.1]\n","            x0 = (1.05)*np.array([1.243,0.226,1.164])*self.alpha#[9,0.5,1]\n","        if np.sum(p0==None) and np.sum(p1==None):\n","            p0 = self.fitted2.x[0:3]\n","            p1 = self.fitted2.x[3:6]\n","        theta2 = self.theta2\n","        #theta2 = self.fitted2.x[6:8]\n","        #p0 = [0.03920017, 0.33350266, 0.59020078]\n","        #p1 = [0.18078681, 0.75749297, 0.06072025]\n","        #theta2 = [0.94946915, 0.01199934]\n","        cons= ({'type': 'ineq', 'fun': lambda x: x[2]-x[1] },\\\n","                {'type': 'ineq', 'fun': lambda x: x[0]-1e-6 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[1]-1e-6 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[2]-1e-6 },\\\n","                {'type': 'ineq', 'fun': lambda x: 100-x[0]},\\\n","                {'type': 'ineq', 'fun': lambda x: 100-x[1]},\\\n","                {'type': 'ineq', 'fun': lambda x: 100-x[2]})\n","        self.fitted = minimize(\n","            self.loglike, x0=x0,#bounds = bounds, \n","            constraints= cons, args=(p0, p1, theta2))\n","        \n","        # bounds = [(1e-6,15),(1e-6 ,2),(1e-6 ,2)]\n","        # func = lambda parameters: self. loglike(parameters, p0, p1, theta2)\n","        # self.fitted = opt.brute(func, bounds, Ns = 3) # To speed up, Ns=3\n","        \n","\n","\n","\n","#%%\n","import pickle\n","import os\n","if __name__ == \"__main__\":\n","    \n","    print('Reminder1: file group4_new_175.csv should be in the same file directory')\n","    print('Reminder2: the code will take almost 4 hours')\n","    \n","    # a dataframe has two columns, named \"state\" and \"action\".    \n","    \n","    nbus = 3000\n","    time = 100\n","    dim_x = 100\n","    \n","    print('->Date generation')\n","    filename = '/content/drive/MyDrive/2022_SUMMER/Research_Rust_Hidden/Code/'\n","    \n","    dat3 = pd.read_csv(filename+'group4_new_175.csv') \n","    estimation1 = FullLikelihood(dat3,'action','state', np.ones(37),\n","                            dim_x =51,dim_z = 175,time = 117, nbus = 37, alpha =0.01,\n","                            hide_state = True, disp_status = False)\n","    res = estimation1.simulation(dim_x,nbus,time,rand_status = True)\n","    \n","    data_record = [] \n","    data_record.append(res)\n","    \n","    # with open('data/sample1500.txt', \"rb\") as fp:   #Pickling\n","    #     res = pickle.load(fp)    \n","        \n","    # # datagenerate1 = FullLikelihood(res['res'][0],'action','state', res['res'][1]['belief'][:,0],\n","    # #                         dim_x =51,dim_z = 175,time = time, nbus = nbus, \n","    # #                         hide_state = True, disp_status = False)\n","    # # res_add = datagenerate1.simulation(15,nbus,time,rand_status = True)\n","    # # datagenerate1.theta2 = [0.949,0.012]\n","    # # loglike1 = datagenerate1.loglike2(parameters=[0.039,0.333,0.590,0.181,0.757,0.061])\n","    # # loglike2 = datagenerate1.loglike(parameters=[9.243,0.226,1.164],\\\n","    # #                 p0 = [0.039,0.333,0.590],p1=[0.181,0.757,0.061],theta2 = [0.949,0.012])\n","    # # print('true loglike1:' ,loglike1)\n","    # # print('true loglike2', loglike2)\n","    # data_record.append([0.949,0.012])\n","    # # data_record.append(loglike1)\n","    # data_record.append([0.039,0.333,0.590,0.181,0.757,0.061])\n","    # # data_record.append(loglike2)\n","    # data_record.append([9.243,0.226,1.164])\n","    \n","\n","    #     #Record in form: \n","    # #       theta2, logSigma, theta3, success1, ccp, [Rc,r1,r2], success2\n","    \n","    alpha_full = np.array([1,0.1,0.01])\n","    for alpha in alpha_full:\n","        print('->Estimation1 x0 known, alpha = ', alpha)\n","        timeStart = tm.time()\n","        \n","        #Initialize\n","        estimation1 = FullLikelihood(res[0].head(nbus*time),'action','state', res[1]['belief'][0:nbus*time,0],\n","                                dim_x =51,dim_z = 175,time = time, nbus = nbus, alpha =alpha,\n","                                hide_state = True, disp_status = True)\n","        estimation1.fit_likelihood2()\n","        data_record.append(estimation1.fitted2)\n","        print(estimation1.fitted2)\n","        # estimation1.theta2 = [0.949,0.012]\n","        # estimation1.fit_likelihood(p0 = [0.039,0.333,0.590],p1=[0.181,0.757,0.061])\n","        estimation1.fit_likelihood()\n","        data_record.append(estimation1.fitted)\n","        print(estimation1.fitted)\n","    \n","    print(data_record)\n","    with open(filename+'sample4_'+str(dim_x)+'_'+str(nbus)+'_'+str(time)+'.txt', \"wb\") as fp:   #Pickling\n","        res = pickle.dump(data_record,fp)   \n","                    "]}]}