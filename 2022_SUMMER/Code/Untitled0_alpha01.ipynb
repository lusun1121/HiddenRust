{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0_alpha01.ipynb","provenance":[],"authorship_tag":"ABX9TyOPE75ZmdCLwB2i6YYFL4pY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tTM-UtbdzHc8","executionInfo":{"status":"ok","timestamp":1655999688081,"user_tz":300,"elapsed":19737,"user":{"displayName":"Lu Sun","userId":"12220989488268625125"}},"outputId":"c538cac7-9a0f-4905-e738-0af735f4bdad"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pickle\n","\n","filename = '/content/drive/MyDrive/2020_WINTER/Rust/data_final/table2_sample_final.txt'\n","\n","with open(filename,'rb') as fl:\n","  res = pickle.load(fl)"],"metadata":{"id":"Uy3ZtwfbRZDh","executionInfo":{"status":"ok","timestamp":1655999852835,"user_tz":300,"elapsed":1365,"user":{"displayName":"Lu Sun","userId":"12220989488268625125"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["res['res'][1]['action'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rO6AfHzOR_xl","executionInfo":{"status":"ok","timestamp":1655999936391,"user_tz":300,"elapsed":155,"user":{"displayName":"Lu Sun","userId":"12220989488268625125"}},"outputId":"6376c065-20b1-4f19-9fd5-a9e6cad51cb0"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000, 100)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6XaNc3CzypXo","executionInfo":{"status":"ok","timestamp":1655504188875,"user_tz":300,"elapsed":5717007,"user":{"displayName":"Lu Sun","userId":"12220989488268625125"}},"outputId":"d4e2d366-ef80-437e-e615-585ab963ae7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reminder1: file group4_new_175.csv should be in the same file directory\n","Reminder2: the code will take almost 4 hours\n","->Date generation\n","[0.12430000000000002, 0.022600000000000002, 0.1164]\n","generate Q\n","generate dataset\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3000/3000 [00:36<00:00, 83.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["->Estimation1 x0 known, alpha =  1.0\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -269599.03886412206\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -269599.03886412206\n","[0.03900001 0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -269599.0329438927\n","[0.039      0.33300001 0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -269599.0388473821\n","[0.039      0.333      0.59000001 0.181      0.757      0.061\n"," 0.949      0.012     ] -269599.0388473544\n","[0.039      0.333      0.59       0.18100001 0.757      0.061\n"," 0.949      0.012     ] -269599.0388777235\n","[0.039      0.333      0.59       0.181      0.75700001 0.061\n"," 0.949      0.012     ] -269599.0389123376\n","[0.039      0.333      0.59       0.181      0.757      0.06100001\n"," 0.949      0.012     ] -269599.0388804056\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.94900001 0.012     ] -269599.0388845912\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.01200001] -269599.0388477339\n","Optimization terminated successfully.    (Exit mode 0)\n","            Current function value: 269599.03886412206\n","            Iterations: 5\n","            Function evaluations: 10\n","            Gradient evaluations: 1\n","     fun: 269599.03886412206\n","     jac: array([-397299.8671875 ,   -1123.3984375 ,   -1125.2578125 ,\n","           912.77734375,    3235.69140625,    1092.76953125,\n","          1373.66015625,   -1099.79296875])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012])\n","[1.30515 0.2373  1.2222 ] -160743.04033790634\n","[1.30515 0.2373  1.2222 ] -160743.04033790634\n","[1.30515001 0.2373     1.2222    ] -160743.04038387648\n","[1.30515    0.23730001 1.2222    ] -160743.04033712862\n","[1.30515    0.2373     1.22220001] -160743.04033761652\n","[1.57047119e-05 3.65500624e+01 3.65500626e+01] -263705.67926050903\n","[1.17463657 3.86857624 4.75498626] -162101.98489587993\n","[1.28379852 0.83136227 1.8001497 ] -160700.51659833963\n","[1.28379852 0.83136227 1.8001497 ] -160700.51659833963\n","[1.28379854 0.83136227 1.8001497 ] -160700.51661616715\n","[1.28379852 0.83136228 1.8001497 ] -160700.51659869138\n","[1.28379852 0.83136227 1.80014972] -160700.51659915087\n","[ 1.13705802e+00 -2.08819911e-06 -2.01164670e-06] -160908.6545398392\n","[1.24088995 0.58826196 1.27376506] -160655.6517095303\n","[1.24088995 0.58826196 1.27376506] -160655.6517095303\n","[1.24088996 0.58826196 1.27376506] -160655.6517021737\n","[1.24088995 0.58826198 1.27376506] -160655.65170995437\n","[1.24088995 0.58826196 1.27376507] -160655.65171002562\n","[1.26836439e+00 9.99022229e-07 9.99948293e-07] -160750.09271258887\n","[1.24686259 0.46038059 0.99686295] -160647.03326937032\n","[1.24686259 0.46038059 0.99686295] -160647.03326937032\n","[1.2468626  0.46038059 0.99686295] -160647.0332705646\n","[1.24686259 0.4603806  0.99686295] -160647.03326946445\n","[1.24686259 0.46038059 0.99686297] -160647.0332694209\n","[1.23511876e+00 9.99988131e-07 1.09499648e+00] -160649.5517094766\n","[1.24344126 0.32625838 1.02545216] -160646.51198064978\n","[1.24344126 0.32625838 1.02545216] -160646.51198064978\n","[1.24344128 0.32625838 1.02545216] -160646.5119806672\n","[1.24344126 0.32625839 1.02545216] -160646.51198066198\n","[1.24344126 0.32625838 1.02545217] -160646.51198070197\n","[1.2267963  0.49365609 0.49365609] -160659.7130606864\n","[1.24177677 0.34299815 0.97227255] -160646.48218718095\n","[1.24177677 0.34299815 0.97227255] -160646.48218718095\n","[1.24177678 0.34299815 0.97227255] -160646.48218617588\n","[1.24177677 0.34299816 0.97227255] -160646.4821872059\n","[1.24177677 0.34299815 0.97227257] -160646.482187188\n","[1.24285017 0.31965206 0.97937878] -160646.43653068412\n","[1.24285017 0.31965206 0.97937878] -160646.43653068412\n","[1.24285019 0.31965206 0.97937878] -160646.43653073878\n","[1.24285017 0.31965208 0.97937878] -160646.43653067437\n","[1.24285017 0.31965206 0.97937879] -160646.43653067926\n","[1.24308874 0.33386621 0.98034089] -160646.43385414066\n","[1.24308874 0.33386621 0.98034089] -160646.43385414066\n","[1.24308875 0.33386621 0.98034089] -160646.43385418903\n","[1.24308874 0.33386623 0.98034089] -160646.43385414328\n","[1.24308874 0.33386621 0.9803409 ] -160646.43385414188\n","[1.24294985 0.32994216 0.97996548] -160646.4332715709\n","[1.24294985 0.32994216 0.97996548] -160646.4332715709\n","[1.24294986 0.32994216 0.97996548] -160646.43327156582\n","[1.24294985 0.32994217 0.97996548] -160646.433271571\n","[1.24294985 0.32994216 0.9799655 ] -160646.43327157092\n","[1.24295629 0.32987915 0.98000653] -160646.4332706608\n","     fun: 160646.4332706608\n","     jac: array([-0.33984375,  0.0078125 ,  0.00195312])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 52\n","     nit: 9\n","    njev: 9\n","  status: 0\n"," success: True\n","       x: array([1.24295629, 0.32987915, 0.98000653])\n","->Estimation1 x0 known, alpha =  0.1\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -269599.03886412206\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -269599.03886412206\n","[0.03900001 0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -269599.0329438927\n","[0.039      0.33300001 0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -269599.0388473821\n","[0.039      0.333      0.59000001 0.181      0.757      0.061\n"," 0.949      0.012     ] -269599.0388473544\n","[0.039      0.333      0.59       0.18100001 0.757      0.061\n"," 0.949      0.012     ] -269599.0388777235\n","[0.039      0.333      0.59       0.181      0.75700001 0.061\n"," 0.949      0.012     ] -269599.0389123376\n","[0.039      0.333      0.59       0.181      0.757      0.06100001\n"," 0.949      0.012     ] -269599.0388804056\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.94900001 0.012     ] -269599.0388845912\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.01200001] -269599.0388477339\n","Optimization terminated successfully.    (Exit mode 0)\n","            Current function value: 269599.03886412206\n","            Iterations: 5\n","            Function evaluations: 10\n","            Gradient evaluations: 1\n","     fun: 269599.03886412206\n","     jac: array([-397299.8671875 ,   -1123.3984375 ,   -1125.2578125 ,\n","           912.77734375,    3235.69140625,    1092.76953125,\n","          1373.66015625,   -1099.79296875])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012])\n","[0.130515 0.02373  0.12222 ] -160743.04033790634\n","[0.130515 0.02373  0.12222 ] -160743.04033790634\n","[0.13051501 0.02373    0.12222   ] -160743.04079760818\n","[0.130515   0.02373001 0.12222   ] -160743.0403301291\n","[0.130515   0.02373    0.12222001] -160743.04033500803\n","[-5.03217461e-03  1.00000069e+02  1.00000015e+02] -1992403.4152381157\n","[ 0.11696028 10.02136388 10.10999954] -284489.9124652456\n","[0.12915953 1.02349339 1.12099795] -165619.03894496663\n","[0.13037945 0.12370634 0.2220978 ] -160771.01788086927\n","[0.1304655  0.06024021 0.15869423] -160730.20345600726\n","[0.1304655  0.06024021 0.15869423] -160730.20345600726\n","[0.13046551 0.06024021 0.15869423] -160730.20383426067\n","[0.1304655  0.06024023 0.15869423] -160730.20345387436\n","[0.1304655  0.06024021 0.15869424] -160730.20345921878\n","[0.0938974  0.02907097 0.02910129] -163058.1939109242\n","[0.12515976 0.05571781 0.13989133] -160656.62290564185\n","[0.12515976 0.05571781 0.13989133] -160656.62290564185\n","[0.12515978 0.05571781 0.13989133] -160656.62290735057\n","[0.12515976 0.05571782 0.13989133] -160656.62290881018\n","[0.12515976 0.05571781 0.13989135] -160656.62291097816\n","[ 1.23562333e-01 -1.44176569e-05 -8.48578588e-06] -160704.16946042835\n","[0.12470743 0.03993663 0.1002772 ] -160646.81448428493\n","[0.12470743 0.03993663 0.1002772 ] -160646.81448428493\n","[0.12470745 0.03993663 0.1002772 ] -160646.81450505054\n","[0.12470743 0.03993665 0.1002772 ] -160646.814484599\n","[0.12470743 0.03993663 0.10027722] -160646.81448455318\n","[1.23853085e-01 1.00167607e-06 1.15874042e-01] -160649.46847649926\n","[0.1245376  0.03199799 0.10337763] -160646.63816625005\n","[0.1245376  0.03199799 0.10337763] -160646.63816625005\n","[0.12453762 0.03199799 0.10337763] -160646.63818134496\n","[0.1245376  0.03199801 0.10337763] -160646.638166101\n","[0.1245376  0.03199799 0.10337765] -160646.63816667648\n","[0.12004261 0.04237464 0.04237464] -160695.21135866534\n","[0.1240881  0.03303566 0.09727733] -160646.5383767146\n","[0.1240881  0.03303566 0.09727733] -160646.5383767146\n","[0.12408812 0.03303566 0.09727733] -160646.53836133817\n","[0.1240881  0.03303567 0.09727733] -160646.53837694827\n","[0.1240881  0.03303566 0.09727735] -160646.53837680843\n","[0.12508523 0.07476988 0.07476988] -160651.57802126487\n","[0.12418781 0.03720908 0.09502659] -160646.5425560676\n","[0.12413422 0.03496607 0.09623625] -160646.52643814805\n","[0.12413422 0.03496607 0.09623625] -160646.52643814805\n","[0.12413424 0.03496607 0.09623625] -160646.52642478602\n","[0.12413422 0.03496609 0.09623625] -160646.52643847454\n","[0.12413422 0.03496607 0.09623627] -160646.52643816127\n","[0.12469812 0.02798815 0.10229557] -160647.0178999368\n","[0.12429511 0.03297514 0.09796509] -160646.4332754775\n","[0.12429511 0.03297514 0.09796509] -160646.4332754775\n","[0.12429513 0.03297514 0.09796509] -160646.43327549004\n","[0.12429511 0.03297516 0.09796509] -160646.43327547482\n","[0.12429511 0.03297514 0.0979651 ] -160646.43327547348\n","[0.12429561 0.03299488 0.09799508] -160646.43327043948\n","[0.12429561 0.03299488 0.09799508] -160646.43327043948\n","[0.12429562 0.03299488 0.09799508] -160646.4332704383\n","[0.12429561 0.0329949  0.09799508] -160646.4332704398\n","[0.12429561 0.03299488 0.0979951 ] -160646.4332704399\n","     fun: 160646.43327043948\n","     jac: array([-0.078125  ,  0.02148438,  0.02929688])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 56\n","     nit: 9\n","    njev: 9\n","  status: 0\n"," success: True\n","       x: array([0.12429561, 0.03299488, 0.09799508])\n","->Estimation1 x0 known, alpha =  0.01\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -269599.03886412206\n","[0.039 0.333 0.59  0.181 0.757 0.061 0.949 0.012] -269599.03886412206\n","[0.03900001 0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -269599.0329438927\n","[0.039      0.33300001 0.59       0.181      0.757      0.061\n"," 0.949      0.012     ] -269599.0388473821\n","[0.039      0.333      0.59000001 0.181      0.757      0.061\n"," 0.949      0.012     ] -269599.0388473544\n","[0.039      0.333      0.59       0.18100001 0.757      0.061\n"," 0.949      0.012     ] -269599.0388777235\n","[0.039      0.333      0.59       0.181      0.75700001 0.061\n"," 0.949      0.012     ] -269599.0389123376\n","[0.039      0.333      0.59       0.181      0.757      0.06100001\n"," 0.949      0.012     ] -269599.0388804056\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.94900001 0.012     ] -269599.0388845912\n","[0.039      0.333      0.59       0.181      0.757      0.061\n"," 0.949      0.01200001] -269599.0388477339\n","Optimization terminated successfully.    (Exit mode 0)\n","            Current function value: 269599.03886412206\n","            Iterations: 5\n","            Function evaluations: 10\n","            Gradient evaluations: 1\n","     fun: 269599.03886412206\n","     jac: array([-397299.8671875 ,   -1123.3984375 ,   -1125.2578125 ,\n","           912.77734375,    3235.69140625,    1092.76953125,\n","          1373.66015625,   -1099.79296875])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012])\n","[0.0130515 0.002373  0.012222 ] -160743.04033790634\n","[0.0130515 0.002373  0.012222 ] -160743.04033790634\n","[0.01305151 0.002373   0.012222  ] -160743.04493497385\n","[0.0130515  0.00237301 0.012222  ] -160743.04026013377\n","[0.0130515  0.002373   0.01222201] -160743.0403089239\n","[ 0.51213883 99.99150258 99.99693744] -inf\n","[ 0.06296023 10.00128596 10.01069354] -1534589.6185901552\n","[0.01804237 1.0022643  1.01206915] -263173.3567518936\n","[0.01355059 0.10236213 0.11220672] -165247.67864082239\n","[0.01310141 0.01237191 0.02222047] -160782.5500386193\n","[0.01306616 0.00530943 0.0151583 ] -160735.47581253265\n","[0.01306616 0.00530943 0.0151583 ] -160735.47581253265\n","[0.01306617 0.00530943 0.0151583 ] -160735.47988864273\n","[0.01306616 0.00530944 0.0151583 ] -160735.47577834447\n","[0.01306616 0.00530943 0.01515831] -160735.47583181167\n","[-0.75552632 -4.32177117 -4.33117609] -16725282.5290763\n","[-0.06379309 -0.42739863 -0.41947514] -1406844.2802710615\n","[ 0.00538023 -0.03796138 -0.02830505] -173108.6445530878\n","[0.01229756 0.00098235 0.01081196] -160650.6838465603\n","[0.01229756 0.00098235 0.01081196] -160650.6838465603\n","[0.01229758 0.00098235 0.01081196] -160650.68303569715\n","[0.01229756 0.00098236 0.01081196] -160650.6838436973\n","[0.01229756 0.00098235 0.01081198] -160650.68385956998\n","[1.24288957e-02 1.14900891e-06 8.77450859e-07] -160709.52119460775\n","[0.01231188 0.00087543 0.00963389] -160649.6994286703\n","[0.01231188 0.00087543 0.00963389] -160649.6994286703\n","[0.01231189 0.00087543 0.00963389] -160649.69884668718\n","[0.01231188 0.00087544 0.00963389] -160649.69941684592\n","[0.01231188 0.00087543 0.0096339 ] -160649.6994248529\n","[0.02602315 0.12851761 0.12851761] -186532.97412215782\n","[0.013683   0.01363964 0.02152226] -160974.35645979067\n","[0.01244899 0.00215185 0.01082273] -160647.13294825863\n","[0.01244899 0.00215185 0.01082273] -160647.13294825863\n","[0.012449   0.00215185 0.01082273] -160647.13315382262\n","[0.01244899 0.00215186 0.01082273] -160647.1329386268\n","[0.01244899 0.00215185 0.01082274] -160647.13295391918\n","[0.01243802 0.00793619 0.00793619] -160652.31021163185\n","[0.0124463  0.00357082 0.01011462] -160646.53236529237\n","[0.0124463  0.00357082 0.01011462] -160646.53236529237\n","[0.01244631 0.00357082 0.01011462] -160646.5324314971\n","[0.0124463  0.00357083 0.01011462] -160646.53236755411\n","[0.0124463  0.00357082 0.01011464] -160646.53236918664\n","[1.20632718e-02 9.99999973e-07 1.09515115e-02] -160676.59235796603\n","[0.012408   0.00321384 0.01019831] -160646.64846609836\n","[0.01243413 0.00345737 0.01014122] -160646.50020501006\n","[0.01243413 0.00345737 0.01014122] -160646.50020501006\n","[0.01243414 0.00345737 0.01014122] -160646.50018981707\n","[0.01243413 0.00345739 0.01014122] -160646.50020776223\n","[0.01243413 0.00345737 0.01014123] -160646.5002097637\n","[0.01241796 0.00675613 0.00675613] -160651.4056344553\n","[0.01243251 0.00378725 0.00980271] -160646.50751736775\n","[0.01243343 0.00359944 0.00999544] -160646.49041810157\n","[0.01243343 0.00359944 0.00999544] -160646.49041810157\n","[0.01243344 0.00359944 0.00999544] -160646.49039236613\n","[0.01243343 0.00359945 0.00999544] -160646.4904217166\n","[0.01243343 0.00359944 0.00999545] -160646.49042174878\n","[0.01242768 0.00316457 0.00971094] -160646.4447595928\n","[0.01242768 0.00316457 0.00971094] -160646.4447595928\n","[0.01242769 0.00316457 0.00971094] -160646.44477020524\n","[0.01242768 0.00316458 0.00971094] -160646.44475797607\n","[0.01242768 0.00316457 0.00971096] -160646.44475795483\n","[0.01242956 0.00329987 0.00979959] -160646.4332706087\n","[0.01242956 0.00329987 0.00979959] -160646.4332706087\n","[0.01242957 0.00329987 0.00979959] -160646.4332705704\n","[0.01242956 0.00329989 0.00979959] -160646.43327061608\n","[0.01242956 0.00329987 0.00979961] -160646.43327061614\n","     fun: 160646.4332706087\n","     jac: array([-2.5703125 ,  0.49609375,  0.5       ])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 66\n","     nit: 10\n","    njev: 10\n","  status: 0\n"," success: True\n","       x: array([0.01242956, 0.00329987, 0.00979959])\n","[(        state  action\n","0          25       0\n","1          25       0\n","2          27       1\n","3           0       0\n","4           2       0\n","...       ...     ...\n","299995      8       0\n","299996      9       0\n","299997     10       0\n","299998     12       0\n","299999     13       0\n","\n","[300000 rows x 2 columns], {'state': array([[ 25,  25,  27, ...,  14,   0,   0],\n","       [137, 138,   0, ...,   5,   7,   9],\n","       [  0,   2,   3, ...,   5,   6,   8],\n","       ...,\n","       [ 64,  65,  65, ...,  11,  12,  13],\n","       [111, 112, 112, ...,   0,   2,   4],\n","       [108, 110,   0, ...,  10,  12,  13]]), 'action': array([[0, 0, 1, ..., 1, 1, 1],\n","       [0, 1, 1, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 1, 0, ..., 0, 0, 0]]), 'hidden': array([[1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       ...,\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0]]), 'belief': array([[0.        , 0.012     , 0.1105027 , ..., 0.93855387, 1.        ,\n","        1.        ],\n","       [0.        , 0.012     , 1.        , ..., 0.94767674, 0.94368161,\n","        0.94325393],\n","       [0.        , 0.012     , 0.01697964, ..., 0.9318202 , 0.81537345,\n","        0.92756591],\n","       ...,\n","       [1.        , 0.949     , 0.76195258, ..., 0.9303987 , 0.81281425,\n","        0.62702318],\n","       [1.        , 0.949     , 0.76195258, ..., 1.        , 0.949     ,\n","        0.94382256],\n","       [1.        , 0.949     , 1.        , ..., 0.6270598 , 0.89472104,\n","        0.75125703]]), 'pis': 0.5}),      fun: 269599.03886412206\n","     jac: array([-397299.8671875 ,   -1123.3984375 ,   -1125.2578125 ,\n","           912.77734375,    3235.69140625,    1092.76953125,\n","          1373.66015625,   -1099.79296875])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012]),      fun: 160646.4332706608\n","     jac: array([-0.33984375,  0.0078125 ,  0.00195312])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 52\n","     nit: 9\n","    njev: 9\n","  status: 0\n"," success: True\n","       x: array([1.24295629, 0.32987915, 0.98000653]),      fun: 269599.03886412206\n","     jac: array([-397299.8671875 ,   -1123.3984375 ,   -1125.2578125 ,\n","           912.77734375,    3235.69140625,    1092.76953125,\n","          1373.66015625,   -1099.79296875])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012]),      fun: 160646.43327043948\n","     jac: array([-0.078125  ,  0.02148438,  0.02929688])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 56\n","     nit: 9\n","    njev: 9\n","  status: 0\n"," success: True\n","       x: array([0.12429561, 0.03299488, 0.09799508]),      fun: 269599.03886412206\n","     jac: array([-397299.8671875 ,   -1123.3984375 ,   -1125.2578125 ,\n","           912.77734375,    3235.69140625,    1092.76953125,\n","          1373.66015625,   -1099.79296875])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 10\n","     nit: 5\n","    njev: 1\n","  status: 0\n"," success: True\n","       x: array([0.039, 0.333, 0.59 , 0.181, 0.757, 0.061, 0.949, 0.012]),      fun: 160646.4332706087\n","     jac: array([-2.5703125 ,  0.49609375,  0.5       ])\n"," message: 'Optimization terminated successfully.'\n","    nfev: 66\n","     nit: 10\n","    njev: 10\n","  status: 0\n"," success: True\n","       x: array([0.01242956, 0.00329987, 0.00979959])]\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat Nov 14 00:47:19 2020\n","11:44\n","@author: Sunlu\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","import scipy.optimize as opt\n","from scipy.optimize import minimize #scipy version = 1.4.1\n","import time as tm\n","#import matplotlib.pyplot as plt\n","#import seaborn as sns\n","import pandas as pd\n","from scipy.misc import derivative\n","import warnings\n","\n","from tqdm import tqdm\n","warnings.filterwarnings(\"ignore\")\n","# warnings.filterwarnings(\"error\")\n","#\n","\n","#%%\n","# def partial_derivative(func, point):\n","#     tol = len(point)\n","#     def wraps(x,var):\n","#         args = point.copy()#[:]\n","#         args[var] = x\n","#         #print(args)\n","#         return func(args)\n","    \n","#     pd = []\n","#     for var in range(tol):\n","#         wraps_new = lambda x: wraps(x,var)\n","#         pd.append(derivative(wraps_new,point[var]))\n","        \n","#     return \n","\n","class FullLikelihood(object):\n","    def __init__(self, data, Y, X,belief0, dim_x, dim_z,time, nbus, alpha,hide_state = True, disp_status = False):\n","        \"\"\"\n","        A statistics workbench used to evaluate the cost parameters underlying \n","        a bus replacement pattern by a forward-looking agent.\n","        \n","        Input:\n","            \n","            * data: a Pandas dataframe, which contains:\n","                -Y: the name of the column containing the dummy exogenous \n","                    variable (here, the choice)\n","                -X: the name of the column containing the endogenous variable \n","                    (here, the state of the bus)\n","            \n","            * dim_x: The number of belif. Start from 0 to 1.\n","            \n","            * dim_z: The number of observation state bins.\n","            \n","            * time: The length of time horizon of the observations.           \n","            \n","            * nbus: The number of buses.\n","            \n","            * hide_state: Default is True. Decide to use Hidden model or not.\n","            \n","            * disp_status: Default is False. Print the grid of opt process.\n","        \"\"\"        \n","        self.data = np.array(data)\n","        self.alpha = alpha\n","        #self.endog = data.loc[:, Y].values\n","        self.exog = data.loc[:, X].values\n","        self.dim = dim_x     \n","        self.S = dim_z \n","        self.time = time\n","        self.nbus = nbus\n","        self.hide_state = hide_state\n","        self.disp_status = disp_status\n","        self.belief0 = belief0\n","        # To speed up computations and avoid loops when computing the log \n","        # likelihood, we create a few useful matrices here:\n","        self.N = data.loc[:, Y].values.shape[0]\n","        self.Dlength = self.S*self.dim\n","        #length = self.Dlength\n","        \n","        # A (length x 2) matrix indicating the blief of a bus at observation \n","        # state i \n","        self.D = [(i,j/(self.dim-1)) for i in range(self.S) \n","                                     for j in range(self.dim)]\n","        \n","\n","        \n","        # A (length x length) matrix indicating the probability of a bus \n","        # transitioning from a state z to a state z' with centain belif  \n","        # self.regen_mat = np.vstack((np.zeros((self.dim-1, self.Dlength)),\n","        #                             np.ones((1,self.Dlength)),\n","        #                             np.zeros((self.Dlength-self.dim, self.Dlength))))\n","        \n","        # A (2xN) matrix indicating with a dummy the decision taken by the \n","        # agent for each time/bus observation (replace or maintain)        \n","        #self.dec_mat = np.vstack(((1-data.loc[:, Y].values), data.loc[:, Y].values))\n","               \n","    def trans(self, theta2,p0,p1):\n","        \"\"\"\n","        This function generate the transition matrix\n","        \n","        Input:\n","            \n","            * theta2: The true state transition probability\n","            \n","            * p0: The transition probability when true state s = 0 (good)\n","            \n","            * p1: The transition probability when true state s = 1 (bad)\n","        Output:\n","        \n","            * trans_mat: A (length x length) matrix indicating the probability \n","                        of a bus transitioning from a obsevation state z to a \n","                        observation state z' with centain belif in good or bad\n","                        true state s.\n","        \"\"\"\n","        \n","        S = self.S\n","        D = self.D\n","        dim = self.dim\n","        length = self.Dlength\n","        \n","        p_0 = np.array([p0[0],p1[0]])\n","        p_0 = np.around(p_0,3)\n","        p_1 = np.array([p0[1],p1[1]])\n","        p_1 = np.around(p_1,3)\n","        p_2 = np.array([p0[2],p1[2]])\n","        p_2 = np.around(p_2,3)\n","        p_3 = 1-p_0-p_1-p_2\n","        \n","        #self.P = P = np.array((p_0,p_1,p_2,p_3))\n","        P = np.array((p_0,p_1,p_2,p_3))\n","        trans_mat = np.zeros((length, length))\n","        new_x0 = lambda x: np.dot(p_0*np.array([x,1-x]),theta2) / np.dot(p_0,[x,1-x])\n","        new_x1 = lambda x: np.dot(p_1*np.array([x,1-x]),theta2) / np.dot(p_1,[x,1-x])\n","        new_x2 = lambda x: np.dot(p_2*np.array([x,1-x]),theta2) / np.dot(p_2,[x,1-x])\n","        new_x3 = lambda x: np.dot(p_3*np.array([x,1-x]),theta2) / np.dot(p_3,[x,1-x])\n","        \n","        matrix0 = [new_x0(i / (dim-1)) for i in range(dim)]\n","        matrix1 = [new_x1(i / (dim-1)) for i in range(dim)]\n","        matrix2 = [new_x2(i / (dim-1)) for i in range(dim)]\n","        matrix3 = [new_x3(i / (dim-1)) for i in range(dim)]\n","        \n","        #self.matrix = x_matrix = np.vstack((matrix0,matrix1,matrix2,matrix3))\n","        x_matrix = np.vstack((matrix0,matrix1,matrix2,matrix3))\n","        \n","        for i in range(length):\n","            \n","            z_cur = D[i][0]\n","            x_cur = D[i][1]\n","            \n","            for j in range(4):\n","                \n","                if self.hide_state == True:\n","                    x_new = x_matrix[j][int(x_cur*(dim-1))]\n","                else:\n","                    x_new = 1\n","                    \n","                x_f = np.floor((self.dim-1)*x_new)/(self.dim-1)\n","                coe = (x_new-x_f)*(self.dim - 1)\n","                \n","                if z_cur + j < S-1:\n","                    ind = D.index((z_cur + j,x_f))\n","                    trans_mat[ind][i] = (1-coe)*np.dot(P[j],[x_cur,1-x_cur])\n","                    \n","                    if x_f != 1:\n","                        trans_mat[ind+1][i] = coe*np.dot(P[j],[x_cur,1-x_cur])\n","                        \n","                elif z_cur + j >= S-1:\n","                    ind = D.index((S-1,x_f))\n","                    trans_mat[ind][i] += (1-coe)*np.dot(P[j],[x_cur,1-x_cur])\n","                    \n","                    if x_f != 1:\n","                        trans_mat[ind+1][i] += coe*np.dot(P[j],[x_cur,1-x_cur])\n","                        \n","                else:\n","                    pass\n","                \n","        return trans_mat\n","    \n","    def belief(self,theta2,p0,p1, path ,x,data_gene_status = False):\n","        \"\"\"\n","        This function return the belief x\n","        \n","        Input:\n","            \n","            * theta2: The true state transition probability\n","            \n","            * p0: The transition probability when s = 0\n","            \n","            * p1: The transition probability when s = 1\n","            \n","            * path: A path of mileage and replacement/maintenance record data\n","        \n","        Output:\n","            \n","            * x: A list with length 'time'. Record belief of each state z\n","        \"\"\"\n","        if data_gene_status:\n","            #path = [[zold,action],[znew,0]]\n","            length = 2\n","            #x = [path[0][2]] #return(xold,xnew)           \n","        \n","        else:\n","            length = self.time\n","            #x = [1]\n","        x = [x]\n","        \n","        p_0 = np.array([p0[0],p1[0]])\n","        p_1 = np.array([p0[1],p1[1]])\n","        p_2 = np.array([p0[2],p1[2]])\n","        p_3 = 1-p_0-p_1-p_2\n","        \n","        for i in range(length-1):\n","            \n","            if path[i][1] == 1:\n","                x_new = 1\n","                x.append(x_new)\n","                \n","            else:\n","                x_cur= np.array([x[-1],1-x[-1]])\n","                gap = path[i+1][0]-path[i][0]\n","                \n","                if gap == 0:\n","                    v1 = np.dot(p_0,x_cur)\n","                    v2 = np.dot(p_0,np.diag(x_cur))\n","                    v3 = np.dot(v2,theta2)\n","                    try:                    \n","                        x_new = v3/v1\n","                    except RuntimeWarning:\n","                        x_new = v3/(v1+np.finfo(float).eps)\n","                    x.append(x_new)\n","                    \n","                elif gap == 1:\n","                    v1 = np.dot(p_1,x_cur)\n","                    v2 = np.dot(p_1,np.diag(x_cur))\n","                    v3 = np.dot(v2,theta2)\n","                    try:                    \n","                        x_new = v3/v1\n","                    except RuntimeWarning:\n","                        x_new = v3/(v1+np.finfo(float).eps)\n","                    x.append(x_new)\n","                    \n","                elif gap == 2:\n","                    v1 = np.dot(p_2,x_cur)\n","                    v2 = np.dot(p_2,np.diag(x_cur))\n","                    v3 = np.dot(v2,theta2)\n","                    try:                    \n","                        x_new = v3/v1\n","                    except RuntimeWarning:\n","                        x_new = v3/(v1+np.finfo(float).eps)\n","                    x.append(x_new)\n","                    \n","                else:\n","                    v1 = np.dot(p_3,x_cur)\n","                    v2 = np.dot(p_3,np.diag(x_cur))\n","                    v3 = np.dot(v2,theta2)\n","                    try:                    \n","                        x_new = v3/v1\n","                    except RuntimeWarning:\n","                        x_new = v3/(v1+np.finfo(float).eps)\n","                    x.append(x_new)\n","        return x\n","    \n","    def simulation(self, dim_x0,SampleSize, TimePeriod, reward = [1.243,0.226,1.164],theta2 = [0.949,0.012],\\\n","                    p0 = [0.039,0.333,0.590],p1=[0.181,0.757,0.061],rand_status = False):\n","        reward = [rw*self.alpha for rw in reward]\n","        print(reward)\n","        condition = np.zeros([SampleSize,TimePeriod],dtype = int) #hidden state: 0,1\n","        state = np.zeros([SampleSize,TimePeriod],dtype = int ) #mileage: 175:0,...,174\n","        x = np.ones([SampleSize,TimePeriod])#belief: s = 0 prob # 删除\n","        if rand_status: # s0 z0 x0\n","            x[:,0] = np.kron(np.linspace(0, 1,num= dim_x0),np.ones(int(SampleSize/dim_x0)))\n","        \n","            #x[:,0] = np.kron(np.random.uniform(size = dim_x0),np.ones(int(SampleSize/dim_x0)))\n","            state[:,0] = np.random.choice(np.arange(175,dtype= int),size = SampleSize,p=np.ones(175)/175)#uniform(0,20)\n","            pis =0.5#0.9509143407122231#np.random.uniform()\n","            condition[:,0] = np.random.choice([0,1],size = SampleSize, p = [pis,1-pis])#0000\n","        else:\n","            pis = 1\n","        action = np.zeros([SampleSize,TimePeriod],dtype = int)\n","        rho = 1\n","        print(\"generate Q\")\n","        Q = self.fl_costs( reward, theta2, p0, p1 ) # Q function Q(x,a) --> interpolation \n","        pchoice =self.choice_prob(rho,Q) # z = 0, x= 0, 0.1, 0.2,....,1, z = 1, x = 0,...,1 pi\n","        print(\"generate dataset\")\n","        for nbus in tqdm(range(SampleSize)):\n","            for t in range(TimePeriod-1):\n","                path = np.zeros([2,2])\n","                path[0,0] = state[nbus,t]\n","                #path[0,2] = x[nbus,t] \n","                \n","                fl =  int(np.floor(x[nbus,t]*(self.dim-1)))\n","                cl =  int(np.ceil(x[nbus,t]*(self.dim-1)))\n","                coef = x[nbus,t] * (self.dim-1)-fl\n","                pi0 = pchoice[state[nbus,t] * self.dim + fl,0] * (1-coef) +\\\n","                    pchoice[state[nbus,t] * self.dim + cl,0] *coef\n","                    \n","                action[nbus,t] = np.random.choice([0,1],p=[pi0,1-pi0])\n","                path[0,1] = action[nbus,t]\n","                \n","                if action[nbus,t]==1:\n","                    state[nbus,t+1] = 0\n","                    condition[nbus,t+1] = 0\n","                    path[1,0] = state[nbus,t+1]\n","                else:\n","                    if condition[nbus,t] == 0:\n","                        state[nbus,t+1] = state[nbus,t] + np.random.choice([0,1,2,3],p=[p0[0],p0[1],p0[2],1-np.sum(p0)])\n","                        if state[nbus,t+1]>self.S-1:\n","                            state[nbus,t+1] = self.S-1\n","                            \n","                        condition[nbus,t+1] = np.random.choice([0,1],p=[theta2[0],1-theta2[0]])\n","                        path[1,0] = state[nbus,t+1]  \n","                    else:\n","                        state[nbus,t+1] = state[nbus,t] + np.random.choice([0,1,2,3],p=[p1[0],p1[1],p1[2],1-np.sum(p1)])\n","                        if state[nbus,t+1]>self.S-1:\n","                            state[nbus,t+1] = self.S-1\n","                        condition[nbus,t+1] = np.random.choice([0,1],p=[theta2[1],1-theta2[1]])\n","                        path[1,0] = state[nbus,t+1]\n","                x[nbus,t+1] = self.belief(theta2,p0,p1, path,x[nbus,t] ,data_gene_status = True)[-1]\n","                \n","            fl =  int(np.floor(x[nbus,TimePeriod-1]*(self.dim-1)))\n","            cl =  int(np.ceil(x[nbus,TimePeriod-1]*(self.dim-1)))\n","            coef = x[nbus,TimePeriod-1] * (self.dim-1)-fl\n","            pi0 = pchoice[state[nbus,TimePeriod-1] * self.dim + fl,0] * (1-coef) +\\\n","                pchoice[state[nbus,TimePeriod-1] * self.dim + cl,0] *coef\n","                \n","            action[nbus,TimePeriod-1] = np.random.choice([0,1],p=[pi0,1-pi0])        \n","          \n","        state_new = np.reshape(state,[-1])\n","        action_new = np.reshape(action,[-1])\n","        data = {'state':state_new,'action':action_new}\n","        dat_new = pd.DataFrame(data=data)\n","        \n","        return dat_new,{'state':state,'action':action,'hidden':condition,'belief':x,'pis':pis}\n","    \n","    def myopic_costs(self, params):\n","        \"\"\"\n","        This function computes the myopic expected cost associated with each \n","        decision for each state.\n","\n","        Input:\n","\n","            * params: A vector, to be supplied to the maintenance linear cost \n","            function. The first element of the vector is the replacement cost rc.\n","\n","        Output:\n","            \n","            * A (length x 2) array containing the maintenance and replacement \n","            costs for the z possible states of the bus.\n","        \"\"\"\n","        \n","        length = self.Dlength\n","        D = self.D\n","        rc = params[0]\n","        thetas = np.array(params[1:])\n","        maint_cost = np.array([np.dot(0.001*D[d][0]*thetas,[D[d][1],1-D[d][1]])\n","                               for d in range(0, length)])\n","        repl_cost = [rc for d in range(0, length)]\n","\n","        return  np.vstack((maint_cost, repl_cost)).T\n","     \n","    def fl_costs(self, params, theta2, p0, p1 ,beta=0.9, threading = 1e-3):#beta=0.9999, threading = 3e-3):#beta=0.9999, threading = 0.45):#\n","        \"\"\"\n","        Compute the non-myopic expected value of the agent for each possible \n","        decision and each possible state of the bus, conditional on a vector of \n","        parameters and on the maintenance cost function specified at the \n","        initialization of the DynamicUtility model.\n","\n","        Iterates until the difference in the previously obtained expected value \n","        and the new expected value is smaller than a constant 'threading'.\n","\n","        Input:\n","\n","            * params: A vector params for the cost function\n","\n","            * theta2: Hiden state transition probability\n","\n","            * p0: The transition probability when s = 0\n","\n","            * p1: The transition probability when s = 1\n","\n","            * beta: Default is 0.9999. A discount factor beta (optional)\n","\n","            * threading: Default is 0.45. A convergence threshold (optional)\n","\n","        Output:\n","\n","            * EV_new: A (length x 2) array of forward-looking costs associated \n","                    with each state z and each belief. Definded as Q\n","        \"\"\"\n","\n","        # Initialization of the contraction mapping\n","        #self.EV_myopic =self.myopic_costs(params)\n","        EV_myopic =self.myopic_costs(params)\n","        \n","        EV_new = EV_myopic\n","\n","        EV = np.zeros(EV_new.shape)        \n","        #VError = np.max(abs(EV-EV_new))\n","        VError = np.max(abs(EV-EV_new))/np.max(np.abs(EV))\n","\n","        #print('Initial Error:',VError,np.max(abs(EV_new)))\n","\n","        #self.trans_mat = self.trans(theta2,p0,p1)\n","        trans_mat = self.trans(theta2,p0,p1)\n","        # Contraction mapping Loop\n","        i = 0\n","        length = self.Dlength\n","        \n","        while(VError>threading):         \n","\n","            EV = EV_new\n","            m = EV.min(1).reshape(length, -1)\n","            #ecost = np.log(np.exp(-EV+m).sum(1) )-m.T+0.5772\n","            ecost = self.alpha*(np.log(np.exp((-EV+m)/self.alpha).sum(1) )-m.T/self.alpha+0.5772)\n","          \n","            futil_maint = np.dot(ecost, trans_mat)\n","            futil_repl = np.dot(ecost, np.vstack((np.zeros((self.dim-1, self.Dlength)),\n","                                    np.ones((1,self.Dlength)),\n","                                    np.zeros((self.Dlength-self.dim, self.Dlength)))))\n","            futil = np.vstack((futil_maint, futil_repl)).T\n","            EV_new = EV_myopic - beta * futil\n","\n","            i +=1\n","            # VError = np.max(abs(EV-EV_new))\n","            # print(i,'th Error:',VError,VError/np.max(abs(EV)),np.max(abs(EV_new)))\n","\n","            VError = np.max(abs(EV-EV_new))/np.max(np.abs(EV))\n","            # print(i,'th Error:',VError,np.max(abs(EV-EV_new)),np.max(abs(EV_new)))\n","\n","            if i>1000:\n","                break\n","            \n","        #print('{}, Eorror:'.format(i),VError,np.max(abs(EV_new)))   \n","\n","        return EV_new\n","\n","    def choice_prob(self, rho, cost_array):\n","        \"\"\"\n","        Returns the probability of each choice for each observed state, \n","        conditional on an array of state/decision costs (generated by the \n","        myopic or forward-looking cost functions)\n","        \"\"\"\n","        cost_array = cost_array/rho\n","        cost = cost_array - cost_array.min(1).reshape(self.Dlength, -1)\n","        #util = np.exp(-cost)\n","        util = np.exp(-cost/self.alpha)\n","        pchoice = util / (np.sum(util, 1).reshape(self.Dlength, -1))\n","\n","        return pchoice\n","\n","    # def loglike_full(self,theta2):\n","    #     \"\"\"\n","    #     The log-likelihood of the Dynamic model(theta 2) is estimated.\n","    #     \"\"\"\n","    #     self.theta2 = theta2\n","    #     self.fit_likelihood2()\n","        \n","    #     loglike = -self.fitted2.fun\n","        \n","    #     if self.disp_status:\n","    #         print(theta2,loglike)\n","\n","    #     return -loglike\n","   \n","    def loglike2(self,parameters):\n","        if len(parameters)==8:\n","            theta2 = parameters[6:8]\n","        else:\n","            theta2 = self.theta2\n","        \n","    # def loglike2(self,parameters):\n","    #     \"\"\"\n","    #     The log-likelihood of the Dynamic model(theta 3) is estimated.\n","    #     \"\"\"\n","\n","    #     theta2 = parameters[6:]\n","        p0 = parameters[0:3]\n","        p1 = parameters[3:6]\n","        \n","        prob0 = np.array([p0[0],p1[0]])\n","        prob1 = np.array([p0[1],p1[1]])\n","        prob2 = np.array([p0[2],p1[2]])\n","\n","        prob3 = 1 - prob0 -prob1 - prob2\n","\n","        mileage = self.exog\n","\n","        if self.hide_state == True:\n","            beli = []\n","            \n","            for i in range(self.nbus):\n","                beli = beli + self.belief(theta2,p0,p1,\n","                                self.data[i*self.time:(i+1)*self.time],self.belief0[i])\n","        else:\n","            beli = [1 for s in range(0,self.N)]\n","        \n","        logprob = 0\n","        for i in range(self.N-1):\n","            x_cur = [beli[i], 1-beli[i]]\n","\n","            if mileage[i+1] - mileage[i] == 0:\n","                try:\n","                    logprob += np.log(np.dot(prob0,x_cur))\n","                except RuntimeWarning:\n","                    logprob += np.log(np.finfo(float).eps)\n","            \n","            elif mileage[i+1] - mileage[i] == 1:\n","                try:\n","                    logprob += np.log(np.dot(prob1,x_cur))\n","                except RuntimeWarning:\n","                    logprob += np.log(np.finfo(float).eps)             \n","\n","            elif mileage[i+1] - mileage[i] == 2:\n","                try:\n","                    logprob += np.log(np.dot(prob2,x_cur))\n","                except RuntimeWarning:\n","                    logprob += np.log(np.finfo(float).eps)\n","\n","            elif mileage[i+1] - mileage[i] == 3:\n","                try:\n","                    logprob += np.log(np.dot(prob3,x_cur))\n","                except RuntimeWarning:\n","                    logprob += np.log(np.finfo(float).eps)\n","\n","        #print(f'current likelihood: {logprob}')\n","\n","        if self.disp_status:\n","            print(parameters,logprob)\n","        return -logprob#/self.time/self.nbus\n","\n","    def loglike(self, parameters, p0, p1, theta2):\n","        \"\"\"\n","        The log-likelihood of the reward model is estimated in several steps.\n","\n","        1°) The current parameters are supplied to the contraction mapping \n","            function\n","\n","        2°) The function returns a matrix of decision probabilities for each \n","            state.\n","\n","        3°) This matrix is used to compute the loglikelihood of the \n","            observations\n","\n","        4°) The log-likelihood are then summed accross individuals, and \n","            returned\n","        \"\"\"\n","\n","        params = parameters[0:3]\n","        rho = 1\n","\n","        # p0 = self.fitted2.x[0:3]\n","        # p1 = self.fitted2.x[3:6]\n","        # theta2 = self.theta2\n","        # A (length x 1) matrix        \n","         \n","\n","        beli = []\n","        #self.beli = []\n","\n","        for i in range(self.nbus):\n","            beli = beli + self.belief(theta2,p0,p1,\n","                                        self.data[i*self.time:(i+1)*self.time],self.belief0[i])\n","\n","\n","        # bool_to_int_f = np.array(((np.array((np.ones((self.Dlength,1)) * self.exog.reshape((1,self.N))) == (np.array(self.D).T[0].reshape((self.Dlength,1)) * np.ones((1,self.N))), dtype=int)) + \\\n","        #                           np.array((np.ones((self.Dlength,1)) * np.floor((np.array(self.beli).reshape((1,self.N)))*(self.dim-1)) / (self.dim-1)) == (np.array(self.D).T[1].reshape((self.Dlength,1)) * np.ones((1,self.N))), dtype=int)) == 2, dtype=int)\n","        # bool_to_int_c = np.array(((np.array((np.ones((self.Dlength,1)) * self.exog.reshape((1,self.N))) == (np.array(self.D).T[0].reshape((self.Dlength,1)) * np.ones((1,self.N))), dtype=int)) +\\\n","        #                           np.array((np.ones((self.Dlength,1)) * np.ceil((np.array(self.beli).reshape((1,self.N))) * (self.dim-1)) / (self.dim-1)) == (np.array(self.D).T[1].reshape((self.Dlength,1)) * np.ones((1,self.N))), dtype=int)) == 2, dtype=int)\n","\n","        # #self.state_mat =\n","        \n","        # self.D = [(i,j/(self.dim-1)) for i in range(self.S) \n","        #                              for j in range(self.dim)]\n","        bool_floor = np.int_(np.floor(np.array(beli)*(self.dim-1)))\n","        bool_ceil = np.int_(np.ceil(np.array(beli)*(self.dim-1)))\n","        bool_non0 = np.where((bool_floor-bool_ceil)!=0)\n","        coef_floor = np.zeros(bool_floor.shape)\n","        coef_floor[bool_non0] = (np.array(beli)*(self.dim-1)-bool_ceil)[bool_non0]/(bool_floor-bool_ceil)[bool_non0]\n","        \n","        choiceProb = self.choice_prob(rho,self.fl_costs(params,theta2,p0,p1)) #states,actions\n","        action = np.int_(self.data[:,1])\n","        statef =  np.int_(self.exog*self.dim+bool_floor)\n","        statec = np.int_(self.exog*self.dim+bool_ceil)\n","        logprob = coef_floor * choiceProb[statef,action] + (1-coef_floor)* choiceProb[statec,action] \n","        \n","        logprob = np.sum(np.log(logprob))\n","        \n","        if self.disp_status:\n","            print(parameters,logprob)\n","\n","        #print(f'current likelihood:{parameters} {logprob}')\n","\n","        return -logprob#/self.time/self.nbus\n","    \n","    \n","    \n","    def fit_likelihood2(self):\n","        \"\"\"\n","        estimate the dynamic(theta 3)\n","        \"\"\"\n","        # bounds = [(0.001, 0.998),(0.001, 0.998),(0.001, 0.998),\n","        #           (0.001, 0.998),(0.001, 0.998),(0.001, 0.998),\n","        #           (0,0.99999),(0,0.99999)]\n","        #x0 = [0.1,0.1,0.1,0.1,0.1,0.1, 0.1, 0.1]\n","        x0 = np.array([0.039,0.333,0.590,0.181,0.757,0.061,0.949,0.012])\n","        cons= ({'type': 'ineq', 'fun': lambda x: x[0]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[1]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[2]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[3]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[4]-0.001},\\\n","                {'type': 'ineq', 'fun': lambda x: x[5]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[6]},\\\n","                {'type': 'ineq', 'fun': lambda x: x[7]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[0]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[1]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[2]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[3]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[4]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.998-x[5]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.99999-x[6]},\\\n","                {'type': 'ineq', 'fun': lambda x: 0.99999-x[7]},\\\n","                {'type': 'ineq', 'fun': lambda x: 1-x[0]-x[1]-x[2]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: 1-x[3]-x[4]-x[5]-0.001 },\\\n","                {'type': 'ineq', 'fun': lambda x: -3*x[0]-2*x[1]-x[2]+3*x[3]+2*x[4]+x[5] })\n","        # x0 = [0.1,0.1,0.1,0.1,0.1,0.1]\n","        # cons= ({'type': 'ineq', 'fun': lambda x: x[0]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: x[1]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: x[2]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: x[3]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: x[4]-0.001},\\\n","        #        {'type': 'ineq', 'fun': lambda x: x[5]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[0]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[1]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[2]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[3]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[4]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 0.998-x[5]},\\\n","        #        {'type': 'ineq', 'fun': lambda x: 1-x[0]-x[1]-x[2]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: 1-x[3]-x[4]-x[5]-0.001 },\\\n","        #        {'type': 'ineq', 'fun': lambda x: -3*x[0]-2*x[1]-x[2]+3*x[3]+2*x[4]+x[5] })\n","        self.fitted2 = minimize(\n","            self.loglike2,\n","            x0=x0,#bounds = bounds,\n","            constraints= cons,\n","            options={'disp': True})\n","        if len(x0)==8:\n","            self.theta2 = self.fitted2.x[6:8]\n","        \n","    def fit_likelihood(self, x0=None, bounds=None,p0 =None,p1=None ):\n","        \"\"\"\n","        estimate the reward parameter\n","        \"\"\"\n","        if bounds == None:\n","            bounds = [(1e-6, 100),(1e-6,100),(1e-6,100)]\n","\n","        if x0 == None:\n","            #x0 = [10,1,1,]\n","            #x0 =  [9.36, 0.94, 0.94] # To speed up, choose a closer initial point\n","            #x0 = [0.1, 0.1, 0.1]\n","            x0 = (1.05)*np.array([1.243,0.226,1.164])*self.alpha#[9,0.5,1]\n","        if np.sum(p0==None) and np.sum(p1==None):\n","            p0 = self.fitted2.x[0:3]\n","            p1 = self.fitted2.x[3:6]\n","        theta2 = self.theta2\n","        #theta2 = self.fitted2.x[6:8]\n","        #p0 = [0.03920017, 0.33350266, 0.59020078]\n","        #p1 = [0.18078681, 0.75749297, 0.06072025]\n","        #theta2 = [0.94946915, 0.01199934]\n","        cons= ({'type': 'ineq', 'fun': lambda x: x[2]-x[1] },\\\n","                {'type': 'ineq', 'fun': lambda x: x[0]-1e-6 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[1]-1e-6 },\\\n","                {'type': 'ineq', 'fun': lambda x: x[2]-1e-6 },\\\n","                {'type': 'ineq', 'fun': lambda x: 100-x[0]},\\\n","                {'type': 'ineq', 'fun': lambda x: 100-x[1]},\\\n","                {'type': 'ineq', 'fun': lambda x: 100-x[2]})\n","        self.fitted = minimize(\n","            self.loglike, x0=x0,#bounds = bounds, \n","            constraints= cons, args=(p0, p1, theta2))\n","        \n","        # bounds = [(1e-6,15),(1e-6 ,2),(1e-6 ,2)]\n","        # func = lambda parameters: self. loglike(parameters, p0, p1, theta2)\n","        # self.fitted = opt.brute(func, bounds, Ns = 3) # To speed up, Ns=3\n","        \n","\n","\n","\n","#%%\n","import pickle\n","import os\n","if __name__ == \"__main__\":\n","    \n","    print('Reminder1: file group4_new_175.csv should be in the same file directory')\n","    print('Reminder2: the code will take almost 4 hours')\n","    \n","    # a dataframe has two columns, named \"state\" and \"action\".    \n","    \n","    nbus = 3000\n","    time = 100\n","    dim_x = 100\n","    \n","    print('->Date generation')\n","    filename = '/content/drive/MyDrive/2022_SUMMER/Research_Rust_Hidden/Code/'\n","    \n","    dat3 = pd.read_csv(filename+'group4_new_175.csv') \n","    estimation1 = FullLikelihood(dat3,'action','state', np.ones(37),\n","                            dim_x =51,dim_z = 175,time = 117, nbus = 37, alpha =0.1,\n","                            hide_state = True, disp_status = False)\n","    res = estimation1.simulation(dim_x,nbus,time,rand_status = True)\n","    \n","    data_record = [] \n","    data_record.append(res)\n","    \n","    # with open('data/sample1500.txt', \"rb\") as fp:   #Pickling\n","    #     res = pickle.load(fp)    \n","        \n","    # # datagenerate1 = FullLikelihood(res['res'][0],'action','state', res['res'][1]['belief'][:,0],\n","    # #                         dim_x =51,dim_z = 175,time = time, nbus = nbus, \n","    # #                         hide_state = True, disp_status = False)\n","    # # res_add = datagenerate1.simulation(15,nbus,time,rand_status = True)\n","    # # datagenerate1.theta2 = [0.949,0.012]\n","    # # loglike1 = datagenerate1.loglike2(parameters=[0.039,0.333,0.590,0.181,0.757,0.061])\n","    # # loglike2 = datagenerate1.loglike(parameters=[9.243,0.226,1.164],\\\n","    # #                 p0 = [0.039,0.333,0.590],p1=[0.181,0.757,0.061],theta2 = [0.949,0.012])\n","    # # print('true loglike1:' ,loglike1)\n","    # # print('true loglike2', loglike2)\n","    # data_record.append([0.949,0.012])\n","    # # data_record.append(loglike1)\n","    # data_record.append([0.039,0.333,0.590,0.181,0.757,0.061])\n","    # # data_record.append(loglike2)\n","    # data_record.append([9.243,0.226,1.164])\n","    \n","\n","    #     #Record in form: \n","    # #       theta2, logSigma, theta3, success1, ccp, [Rc,r1,r2], success2\n","    \n","    alpha_full = np.array([1,0.1,0.01])\n","    for alpha in alpha_full:\n","        print('->Estimation1 x0 known, alpha = ', alpha)\n","        timeStart = tm.time()\n","        \n","        #Initialize\n","        estimation1 = FullLikelihood(res[0].head(nbus*time),'action','state', res[1]['belief'][0:nbus*time,0],\n","                                dim_x =51,dim_z = 175,time = time, nbus = nbus, alpha =alpha,\n","                                hide_state = True, disp_status = True)\n","        estimation1.fit_likelihood2()\n","        data_record.append(estimation1.fitted2)\n","        print(estimation1.fitted2)\n","        # estimation1.theta2 = [0.949,0.012]\n","        # estimation1.fit_likelihood(p0 = [0.039,0.333,0.590],p1=[0.181,0.757,0.061])\n","        estimation1.fit_likelihood()\n","        data_record.append(estimation1.fitted)\n","        print(estimation1.fitted)\n","    \n","    print(data_record)\n","    with open(filename+'sample3_'+str(dim_x)+'_'+str(nbus)+'_'+str(time)+'.txt', \"wb\") as fp:   #Pickling\n","        res = pickle.dump(data_record,fp)   \n","                    "]}]}